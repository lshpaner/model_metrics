{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import model_metrics\n",
    "\n",
    "print(f\"Model Metrics version: {model_metrics.__version__}\")\n",
    "\n",
    "from model_tuner import loadObjects\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from eda_toolkit import ensure_directory\n",
    "\n",
    "print(f\"Python version: {os.sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Metrics version: {model_metrics.__version__}\")\n",
    "print(f\"Model Metrics authors: {model_metrics.__author__} \\n\")\n",
    "\n",
    "## Define base paths\n",
    "## `base_path`` represents the parent directory of current working directory\n",
    "base_path = os.path.join(os.pardir)\n",
    "## Go up one level from 'notebooks' to the parent directory, then into the\n",
    "## 'results' folder\n",
    "\n",
    "model_path = os.path.join(os.pardir, \"model_files/results\")\n",
    "data_path = os.path.join(os.pardir, \"model_files\")\n",
    "image_path_png = os.path.join(data_path, \"images\", \"png_images\")\n",
    "image_path_svg = os.path.join(data_path, \"images\", \"svg_images\")\n",
    "\n",
    "# Use the function to ensure the 'data' directory exists\n",
    "ensure_directory(model_path)\n",
    "ensure_directory(image_path_png)\n",
    "ensure_directory(image_path_svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a synthetic dataset for classification using `make_classification` from `sklearn.datasets`.\n",
    "This dataset will be used to train and evaluate multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train models\n",
    "model1 = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
    "model2 = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "model_titles = [\"Logistic Regression\", \"Random Forest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Predictions from Adult Income Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X_ai = adult.data.features\n",
    "\n",
    "# X.to_csv(\"../data/X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = loadObjects(os.path.join(model_path, \"LogisticRegression.pkl\"))\n",
    "model_dt = loadObjects(os.path.join(model_path, \"DecisionTreeClassifier.pkl\"))\n",
    "model_rf = loadObjects(os.path.join(model_path, \"RandomForestClassifier.pkl\"))\n",
    "\n",
    "X_test_ai = pd.read_parquet(os.path.join(data_path, \"X_test.parquet\"))\n",
    "y_test_ai = pd.read_parquet(os.path.join(data_path, \"y_test.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set The Desired Naming Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_or_models = [\n",
    "    model_lr[\"model\"].estimator,\n",
    "    model_rf[\"model\"].estimator,\n",
    "    model_dt[\"model\"].estimator,\n",
    "]\n",
    "\n",
    "# Model titles\n",
    "model_titles_ai = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Random Forest Classifier\",\n",
    "    \"Decision Tree Classifier\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2_ai = X_test_ai.join(\n",
    "    X_ai[[\"sex\", \"race\", \"relationship\", \"occupation\", \"workclass\", \"education\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Example 1: Default Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we summarize the performance of multiple models using the default threshold of 0.5 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "model_performance = summarize_model_performance(\n",
    "    model=[model1, model2],\n",
    "    model_title=model_titles,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_type=\"classification\",\n",
    "    return_df=True,\n",
    ")\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Example 2: Custom Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we revisit binary classification with the same two models: Logistic Regression and Random Forest, but adjust the classification threshold (`custom_threshold` input in this case) from the default 0.5 to 0.2. This change allows us to explore how lowering the threshold impacts model performance, potentially increasing sensitivity (recall) by classifying more instances as positive (1) at the expense of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "model_performance = summarize_model_performance(\n",
    "    model=[model1, model2],\n",
    "    model_title=model_titles,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_type=\"classification\",\n",
    "    return_df=True,\n",
    "    custom_threshold=0.2,\n",
    ")\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model Examples w/ Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells, we will demonstrate model performance summarization for regression models using the diabetes dataset from `sklearn.datasets`.\n",
    "This dataset will be used to train and evaluate multiple regression models, and we will summarize their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "diabetes = load_diabetes(as_frame=True)[\"frame\"]\n",
    "X_diabetes = diabetes.drop(columns=[\"target\"])\n",
    "y_diabetes = diabetes[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(\n",
    "    X_diabetes,\n",
    "    y_diabetes,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train Linear Regression (on unscaled data)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_diabetes, y_train_diabetes)\n",
    "# Train Random Forest Regressor (on unscaled data)\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    ")\n",
    "rf_model.fit(X_train_diabetes, y_train_diabetes)\n",
    "\n",
    "# Train Ridge Regression (on scaled data)\n",
    "ridge_model = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"estimator\", Ridge(alpha=1.0)),\n",
    "    ]\n",
    ")\n",
    "ridge_model.fit(X_train_diabetes, y_train_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example 1: Summarize Model Performance: Linear, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    return_df=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output below presents a detailed comparison of the performance and coefficients for two regression models: Linear Regression and Ridge Regression trained on the diabetes dataset. It includes overall metrics such as Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Explained Variance, and $R^{2}$ Score for each model, showing their predictive accuracy. Additionally, it lists the coefficients for each feature (e.g., age, bmi, s1–s6) in both models, highlighting how each variable contributes to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example 2: Summarize Model Performance: Linear, Ridge, RF (w/ Feature Importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Regression Example 2, we extend the analysis by introducing a Random Forest Regressor alongside Linear Regression and Ridge Regression to demonstrate how a model with feature importances, rather than coefficients, impacts evaluation outcomes. The code uses the `summarize_model_performance` function from `model_metrics` to assess all three models on the diabetes dataset’s test set, ensuring the Random Forest’s feature importance-based predictions are reflected in the results while preserving the coefficient-based results of the other models, as shown in the subsequent table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    return_df=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example 3: Summarize Model Performance (Adjusted $R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some regression analyses, it is useful to report **Adjusted R²** in addition \n",
    "to standard error and variance metrics. Adjusted R² accounts for the number of \n",
    "predictors in the model and penalizes unnecessary complexity, making it more \n",
    "appropriate than R² when comparing models with different feature counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    return_df=True,\n",
    "    include_adjusted_r2=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example 4 - Summarize Model Performance (Overall Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some scenarios, you may want to simplify the output by excluding variables, coefficients, and feature importances from the model results. This example demonstrates how to achieve that by setting `overall_only=True` in the `summarize_model_performance` function, producing a concise table that focuses on key metrics: model name, Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Explained Variance, and $R^2$ Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    overall_only=True,\n",
    "    return_df=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression example 5: Printed Table with Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Regression Example 5, we illustrate how to generate a printed table that includes the Adjusted $R^2$ metric for regression models. By setting the `include_adjusted_r2` parameter to `True` in the `summarize_model_performance` function, we ensure that the output table provides a comprehensive view of model performance, including both $R^2$ and Adjusted $R^2$ values, which account for the number of predictors in the model. This allows for a more nuanced evaluation of model effectiveness, especially when comparing models with different numbers of features. Here, we toggle `return_df` to `False` (or simply do not pass it) to display the results directly as a printed table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual diagnostics are essential tools for evaluating regression model performance beyond \n",
    "standard metrics like $R^2$ or RMSE. By examining the patterns in residuals: the differences \n",
    "between observed and predicted values, we can identify violations of modeling assumptions, \n",
    "detect systematic errors, and uncover opportunities for model improvement.\n",
    "\n",
    "The `show_residual_diagnostics` function provides comprehensive visualization of residual \n",
    "patterns across multiple dimensions:\n",
    "\n",
    "- **Residuals vs Fitted Values**: Assess homoscedasticity (constant variance) and identify non-linear patterns\n",
    "- **Residuals vs Predictors**: Examine whether specific features are associated with systematic prediction errors\n",
    "- **Q-Q Plots**: Evaluate whether residuals follow a normal distribution\n",
    "- **Histogram of Residuals**: Visualize the distribution shape and identify outliers\n",
    "- **Scale-Location Plots**: Detect heteroscedasticity (non-constant variance)\n",
    "\n",
    "**What Good Residuals Look Like:**\n",
    "\n",
    "- Randomly scattered around zero with no systematic patterns\n",
    "- Constant spread across the range of fitted values (homoscedasticity)\n",
    "- Approximately normally distributed (for inference and prediction intervals)\n",
    "- No strong correlations with individual predictor variables\n",
    "\n",
    "**What Bad Residuals Reveal:**\n",
    "\n",
    "- **Funnel shapes** (heteroscedasticity): Variance increases/decreases with predicted values, suggesting transformations may be needed\n",
    "- **Curved patterns**: Non-linear relationships that the model hasn't captured\n",
    "- **Clusters or groups**: Systematic differences across subpopulations that may require interaction terms or stratified models\n",
    "- **Heavy tails or skewness**: Outliers or violations of normality assumptions\n",
    "- **Patterns vs predictors**: Missing interaction effects or non-linear relationships with specific features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Diagnostics Example 1: Comprehensive Residual Diagnostics Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first example demonstrates a complete residual diagnostic analysis for a single \n",
    "regression model using `plot_type=\"all\"`. This setting generates all available diagnostic \n",
    "visualizations in a single comprehensive display:\n",
    "\n",
    "1. **Residuals vs Fitted Values**: Detects non-linearity, heteroscedasticity, and outliers\n",
    "2. **Q-Q Plot**: Assesses normality of residuals\n",
    "3. **Scale-Location Plot**: Evaluates homoscedasticity (constant variance)\n",
    "4. **Residuals vs Leverage**: Identifies influential observations\n",
    "5. **Histogram of Residuals**: Shows the distribution shape\n",
    "\n",
    "We evaluate a Random Forest model trained on the diabetes dataset. The `n_clusters=3` \n",
    "parameter performs k-means clustering on the residuals to identify groups of observations \n",
    "with similar prediction error patterns. Setting `show_centroids=True` overlays cluster \n",
    "centers on the residual plots, styled with custom colors and markers via `centroid_kwgs`.\n",
    "\n",
    "The `kmeans_rstate=222` parameter controls the random seed for k-means clustering, ensuring \n",
    "reproducible cluster assignments across repeated runs. By default, `kmeans_rstate` is set \n",
    "to 42, making clustering deterministic unless explicitly changed. This is important because \n",
    "k-means uses random initialization; different seeds can produce slightly different cluster \n",
    "assignments, especially when clusters overlap or are of similar size. Setting a fixed seed \n",
    "ensures that diagnostic plots remain consistent for documentation, presentations, and \n",
    "collaborative analysis.\n",
    "\n",
    "To formally test for heteroscedasticity, we enable `heteroskedasticity_test=\"breusch_pagan\"`. \n",
    "This optional parameter runs the Breusch-Pagan test, which evaluates whether residual \n",
    "variance is systematically related to predicted values. Test results, including the test \n",
    "statistic, *p*-value, and interpretation, are printed to the console. A significant result \n",
    "(*p* < 0.05) indicates heteroscedasticity, suggesting that predictions may be more reliable \n",
    "for certain ranges of the response variable than others.\n",
    "\n",
    "Additional customization options include:\n",
    "\n",
    "- `n_cols=2`: Arranges diagnostic plots in a 2-column grid layout\n",
    "- `histogram_type=\"density\"`: Displays residuals as a density plot rather than raw counts\n",
    "- `decimal_places=2`: Controls precision of printed test statistics\n",
    "- `tick_fontsize` and `label_fontsize`: Adjust text sizing for readability\n",
    "- `save_plot=True` with image paths: Exports plots as PNG and SVG for reports\n",
    "\n",
    "The function also returns a diagnostics dictionary containing residuals, fitted values, \n",
    "standardized residuals, and leverage statistics. This allows for programmatic access to \n",
    "diagnostic quantities for custom analyses or integration with `resid_diagnostics_to_dataframe` \n",
    "to convert results into a pandas DataFrame for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pred = linear_model.predict(X_test_diabetes)\n",
    "rf_pred = rf_model.predict(X_test_diabetes)\n",
    "ridge_pred = ridge_model.predict(X_test_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_residual_diagnostics\n",
    "\n",
    "show_residual_diagnostics(\n",
    "    y_pred=rf_pred,\n",
    "    model_title=[\"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    n_clusters=3,\n",
    "    n_cols=2,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    tick_fontsize=12,\n",
    "    label_fontsize=14,\n",
    "    plot_type=\"all\",\n",
    "    show_centroids=True,\n",
    "    centroid_kwgs={\"c\": [\"red\", \"blue\", \"green\"], \"marker\": \"X\", \"s\": 50},\n",
    "    heteroskedasticity_test=\"breusch_pagan\",\n",
    "    decimal_places=2,\n",
    "    histogram_type=\"density\",\n",
    "    kmeans_rstate=222,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Diagnostics Example 2: Singe Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates two key capabilities for focused residual analysis:\n",
    "\n",
    "1. **Selective plot generation**: The `plot_type` parameter allows you to generate specific diagnostic plots rather than the full suite. Pass a single plot name as a string (e.g., `\"fitted\"`) or a list of plot names for multiple specific plots (e.g., `[\"fitted\", \"qq\", \"histogram\"]`). This is useful when you need to examine particular model assumptions or create targeted visualizations for reports.\n",
    "\n",
    "2. **LOWESS trend detection**: Setting `show_lowess=True` adds a locally weighted scatterplot smoothing (LOWESS) curve to residual plots. This non-parametric smoothing line reveals systematic patterns or trends in the residuals that might not be obvious from the scatter alone. If model assumptions hold, the LOWESS line should be roughly horizontal at y=0. Pronounced curves or trends indicate potential violations of linearity or suggest that the model is systematically over- or under-predicting in certain regions.\n",
    "\n",
    "We focus on the Scale-Location plot (`plot_type=\"scale_location\"`), which is particularly useful for detecting heteroscedasticity: the violation of the constant variance assumption. This plot displays the square root of standardized residuals against fitted values, making it easier to spot changes in residual spread across the prediction range. The LOWESS smoothing line, styled in orange via `lowess_kwgs`, helps identify whether variance increases, decreases, or remains stable as predictions change.\n",
    "The `heteroskedasticity_test=\"breusch_pagan\"` parameter formally tests for \n",
    "heteroscedasticity. The Breusch-Pagan test evaluates whether residual variance \n",
    "is systematically related to the predictors or fitted values. Test results appear \n",
    "in the plot legend (if space permits) or can be displayed in a diagnostic table \n",
    "using `show_diagnostics_table=True`. A significant result (*p* < 0.05) provides \n",
    "statistical evidence of heteroscedasticity, which may require remedial measures \n",
    "such as variance-stabilizing transformations, weighted least squares, or robust \n",
    "standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_residual_diagnostics\n",
    "\n",
    "show_residual_diagnostics(\n",
    "    y_pred=rf_pred,\n",
    "    model_title=[\"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    tick_fontsize=12,\n",
    "    label_fontsize=14,\n",
    "    figsize=(10, 8),\n",
    "    plot_type=\"scale_location\",\n",
    "    point_kwgs={\"alpha\": 0.9, \"color\": \"blue\", \"edgecolor\": \"black\", \"s\": 50},\n",
    "    show_lowess=True,\n",
    "    lowess_kwgs={\"color\": \"red\", \"lw\": 2},\n",
    "    show_centroids=True,\n",
    "    heteroskedasticity_test=\"breusch_pagan\",\n",
    "    decimal_places=2,\n",
    "    histogram_type=\"density\",\n",
    "    kmeans_rstate=222,\n",
    "    suptitle=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Diagnostics Example 3: Diagnostics Table Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to generate a comprehensive residual diagnostics summary \n",
    "table **without displaying plots**. By setting `show_plots=False` and \n",
    "`show_diagnostics_table=True`, the function outputs only a tabular summary of key \n",
    "diagnostic statistics and heteroscedasticity test results.\n",
    "\n",
    "The diagnostics table includes:\n",
    "\n",
    "- **Residual statistics**: Mean, standard deviation, min, max, and quartiles\n",
    "- **Standardized residual metrics**: Useful for identifying outliers ($|z| > 3$)\n",
    "- **Heteroscedasticity test results**: When `heteroskedasticity_test` is specified\n",
    "\n",
    "In this example, we set `heteroskedasticity_test=\"all\"` to run **all available tests**:\n",
    "\n",
    "- **Breusch-Pagan**: Tests whether residual variance depends on predicted values\n",
    "- **White**: A more general test that doesn't assume a specific functional form\n",
    "- **Goldfeld-Quandt**: Compares variance between two subsamples\n",
    "\n",
    "Each test returns a test statistic, p-value, and interpretation. The `decimal_places=5` \n",
    "parameter ensures high precision in the printed output, which is useful for reporting \n",
    "results in research papers or technical documentation.\n",
    "\n",
    "The `return_diagnostics=True` parameter returns a dictionary containing all diagnostic \n",
    "quantities (residuals, fitted values, standardized residuals, leverage, etc.) for \n",
    "programmatic access or conversion to a DataFrame using `resid_diagnostics_to_dataframe`.\n",
    "\n",
    "**Note:** You can also display **both the table and plots simultaneously** by setting \n",
    "`show_plots=True` and `show_diagnostics_table=True` together. This provides a \n",
    "comprehensive view combining visual diagnostics with quantitative summaries, ideal for \n",
    "thorough model evaluation reports.\n",
    "\n",
    "Additional parameters used:\n",
    "\n",
    "- `plot_type=\"histogram\"`: Specifies which plot type to generate (only relevant if `show_plots=True`)\n",
    "- `n_clusters=3` and `show_centroids=True`: Configures k-means clustering (applied to returned diagnostics)\n",
    "- `save_plot=True`: Would save plots if `show_plots=True`\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_residual_diagnostics\n",
    "\n",
    "diagnostics = show_residual_diagnostics(\n",
    "    y_pred=rf_pred,\n",
    "    model_title=[\"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    n_clusters=3,\n",
    "    n_cols=2,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    tick_fontsize=12,\n",
    "    label_fontsize=14,\n",
    "    plot_type=\"histogram\",\n",
    "    show_centroids=True,\n",
    "    centroid_kwgs={\"c\": [\"red\", \"blue\", \"green\"], \"marker\": \"X\", \"s\": 50},\n",
    "    heteroskedasticity_test=\"all\",\n",
    "    legend_loc=\"upper right\",\n",
    "    show_diagnostics_table=True,\n",
    "    return_diagnostics=True,\n",
    "    show_plots=False,\n",
    "    decimal_places=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Diagnostics Example 4: Diagnostics to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on Example 2, the diagnostics dictionary returned by `show_residual_diagnostics()` \n",
    "can be converted into a pandas DataFrame for programmatic analysis, reporting, or integration \n",
    "into automated pipelines. \n",
    "\n",
    "The `resid_diagnostics_to_dataframe()` helper function handles \n",
    "this conversion seamlessly, properly flattening nested structures like heteroskedasticity \n",
    "test results. Unlike the console table which only displays *p*-values and interpretations, \n",
    "the DataFrame provides complete test results including both the test statistics and \n",
    "*p*-values: useful for creating custom reports, academic papers, or detailed model \n",
    "documentation that requires full statistical disclosure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import resid_diagnostics_to_dataframe\n",
    "\n",
    "df = resid_diagnostics_to_dataframe(diagnostics)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Diagnostics Example 5: Grouped Analysis with Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particularly powerful extension of residual diagnostics is stratification by categorical \n",
    "variables such as demographic groups, treatment arms, or geographic regions. By examining \n",
    "residuals separately for each subgroup, we can identify whether:\n",
    "\n",
    "- Model performance is consistent across populations\n",
    "- Systematic bias exists for specific groups\n",
    "- Variance differs across subgroups (heteroscedasticity by group)\n",
    "- Interaction effects between predictors and group membership are present\n",
    "\n",
    "This is especially critical in applications where fairness and equity matter such as \n",
    "healthcare, lending, and social services, where models should not systematically  \n",
    "under-predict or over-predict for protected or vulnerable populations.\n",
    "\n",
    "This example demonstrates how to examine residual patterns across categorical subgroups \n",
    "using the `group_category` parameter. By stratifying residual diagnostics by a \n",
    "categorical variable: such as sex, age group, or treatment arm, we can identify whether \n",
    "model errors are consistent across subpopulations or if certain groups exhibit systematic \n",
    "bias or heteroscedasticity.\n",
    "\n",
    "In this example, we evaluate three regression models trained on the diabetes dataset: \n",
    "Linear Regression, Ridge Regression, and Random Forest. The `sex` variable in the \n",
    "original dataset is encoded numerically (positive/negative values), so we first transform \n",
    "it into interpretable categories (\"Male\" and \"Female\") before passing it to \n",
    "`show_residual_diagnostics`.    \n",
    "The `plot_type=\"predictors\"` option generates residual plots for each predictor variable, \n",
    "with points color-coded by the categorical group. This allows us to visually assess whether:\n",
    "\n",
    "- Residuals are centered around zero for both groups\n",
    "- Variance is similar across groups (homoscedasticity)\n",
    "- Any systematic patterns exist that might indicate interaction effects or model misspecification\n",
    "\n",
    "When `show_centroids=True` is enabled, group centroids are overlaid on the plots to \n",
    "highlight the mean residual behavior for each subgroup. The `centroid_kwgs` parameter \n",
    "allows customization of these centroids with specific colors, markers, and sizes to \n",
    "distinguish between groups clearly.\n",
    "\n",
    "This type of analysis is particularly valuable in healthcare and social science applications \n",
    "where fairness and equity are critical concerns. Identifying residual patterns by demographic \n",
    "variables can reveal whether a model's predictions are systematically biased against \n",
    "specific subpopulations, informing decisions about model refinement or the need for \n",
    "group-specific calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'sex' column is already categorical-like (coded as positive/negative values)\n",
    "# Let's make it more interpretable\n",
    "X_test_diab_copy = X_test_diabetes.copy()\n",
    "X_test_diab_copy[\"sex_category\"] = X_test_diab_copy[\"sex\"].apply(\n",
    "    lambda x: \"Male\" if x > 0 else \"Female\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_diab_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_residual_diagnostics\n",
    "\n",
    "show_residual_diagnostics(\n",
    "    y_pred=rf_pred,\n",
    "    model_title=\"Random Forest\",\n",
    "    X=X_test_diab_copy[[\"age\", \"bmi\", \"sex_category\"]],\n",
    "    y=y_test_diabetes,\n",
    "    plot_type=\"predictors\",\n",
    "    show_centroids=True,\n",
    "    centroid_type=\"groups\",\n",
    "    group_category=\"sex_category\",\n",
    "    centroid_kwgs={\"c\": [\"red\", \"blue\"], \"marker\": \"X\", \"s\": 50},\n",
    "    group_kwgs={\n",
    "        \"color\": [\"#1f77b4\", \"#ff7f0e\"],  # Custom hex colors\n",
    "        \"alpha\": 0.8,\n",
    "        \"s\": 60,\n",
    "        \"edgecolors\": \"black\",\n",
    "    },\n",
    "    # legend_loc=\"bottom\",\n",
    "    heteroskedasticity_test=\"all\",\n",
    "    # figsize=(12, 8),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    suptitle=\"\",\n",
    "    decimal_places=2,\n",
    "    kmeans_rstate=222,\n",
    "    # legend_kwgs={\n",
    "    #     \"show_groups\": True,\n",
    "    #     \"show_centroids\": False,\n",
    "    #     \"show_het_tests\": True,\n",
    "    #     \"show_cooks\": True,\n",
    "    # },\n",
    "    # legend_kwgs=False,\n",
    "    legend_loc=\"bottom\",\n",
    "    xlim=(-0.15, 0.15),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Diagnostics Example 6: Multiple Models with Shared Axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to compare residual diagnostics across **multiple models** with \n",
    "**standardized axis limits** for direct visual comparison. By passing a list of \n",
    "predictions and model names, you can evaluate multiple models side-by-side using any \n",
    "diagnostic plot type.\n",
    "\n",
    "Key features demonstrated:\n",
    "\n",
    "**Model Comparison Across Any Plot Type**\n",
    "   While this example uses histograms (`plot_type=\"histogram\"`), the same approach \n",
    "   works for **all diagnostic plot types**:\n",
    "   \n",
    "   - **Residuals vs Fitted** (`plot_type=\"fitted\"`): Compare linearity assumptions\n",
    "   - **Q-Q Plots** (`plot_type=\"qq\"`): Compare normality of residuals\n",
    "   - **Scale-Location** (`plot_type=\"scale_location\"`): Compare homoscedasticity\n",
    "   - **Leverage Plots** (`plot_type=\"leverage\"`): Compare influential observations\n",
    "   - **All plots** (`plot_type=\"all\"`): Generate all 6 diagnostic plots for each model\n",
    "   \n",
    "   Simply change the `plot_type` parameter while keeping the same multi-model structure \n",
    "   (`y_pred=[model1, model2]`) to create comprehensive cross-model comparisons for any \n",
    "   diagnostic.\n",
    "\n",
    "**Shared Axis Limits**\n",
    "   - `xlim=(-175, 175)`: Standardizes the x-axis (residual values) across both models\n",
    "   - `ylim=(0, 10)`: Standardizes the y-axis (frequency counts) across both models\n",
    "   \n",
    "   This ensures that visual differences in residual distributions reflect actual model \n",
    "   performance rather than different axis scales.\n",
    "\n",
    "**Group-Based Analysis**\n",
    "   The `group_category=\"sex_category\"` parameter colors points by sex, with custom \n",
    "   styling via `group_kwgs`. When combined with `show_centroids=True` and \n",
    "   `centroid_type=\"groups\"`, group-specific centroids are displayed to reveal \n",
    "   whether residual patterns differ across demographic groups.\n",
    "\n",
    "**Legend Positioning**\n",
    "   Setting `legend_loc=\"bottom\"` places legends below the x-axis with proper spacing. \n",
    "   The function automatically adds vertical space to accommodate bottom legends when \n",
    "   using default figure sizes.\n",
    "\n",
    "**Comprehensive Heteroscedasticity Testing**\n",
    "   `heteroskedasticity_test=\"all\"` runs all four available tests (Breusch-Pagan, \n",
    "   White, Goldfeld-Quandt, Spearman) and displays results in the legend. This helps \n",
    "   identify whether residual variance is constant across fitted values.\n",
    "\n",
    "**Title and Layout Customization**\n",
    "   - `suptitle=\"\"`: Suppresses the overall figure title for a cleaner look\n",
    "   - `text_wrap=35`: Wraps subplot titles at 35 characters\n",
    "   - `n_cols=2`: Arranges subplots in 2 columns for side-by-side comparison\n",
    "\n",
    "**When to Use Multi-Model Comparison**\n",
    "   Multi-model comparison is particularly valuable when:\n",
    "   \n",
    "   - Comparing different algorithms (e.g., linear vs tree-based models)\n",
    "   - Evaluating hyperparameter tuning results (e.g., different regularization strengths)\n",
    "   - Assessing feature engineering impact (e.g., with vs without transformations)\n",
    "   - Creating model selection documentation for reports or publications\n",
    "\n",
    "**When to Use Shared Axes**\n",
    "   Shared axis limits (`xlim`/ `ylim`) are recommended when models have similar scales. \n",
    "   If models produce residuals on very different scales, omit these parameters to let \n",
    "   each subplot use optimal ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_residual_diagnostics\n",
    "\n",
    "show_residual_diagnostics(\n",
    "    y_pred=[ridge_pred, rf_pred],\n",
    "    model_title=[\"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diab_copy[[\"age\", \"bmi\", \"sex_category\"]],\n",
    "    y=y_test_diabetes,\n",
    "    plot_type=\"histogram\",\n",
    "    # histogram_type=\"density\",\n",
    "    show_centroids=True,\n",
    "    centroid_type=\"groups\",\n",
    "    group_category=\"sex_category\",\n",
    "    centroid_kwgs={\"c\": [\"red\", \"blue\"], \"marker\": \"X\", \"s\": 50},\n",
    "    group_kwgs={\n",
    "        \"color\": [\"#1f77b4\", \"#ff7f0e\"],  # Custom hex colors\n",
    "        \"alpha\": 0.8,\n",
    "        \"s\": 60,\n",
    "        \"edgecolors\": \"black\",\n",
    "    },\n",
    "    # legend_loc=\"bottom\",\n",
    "    heteroskedasticity_test=\"all\",\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    # show_lowess=True,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    suptitle=\"\",\n",
    "    n_cols=2,\n",
    "    kmeans_rstate=222,\n",
    "    # legend_kwgs={\n",
    "    #     \"show_groups\": True,\n",
    "    #     \"show_centroids\": False,\n",
    "    #     \"show_het_tests\": True,\n",
    "    #     \"show_cooks\": True,\n",
    "    # },\n",
    "    # legend_kwgs=False,\n",
    "    text_wrap=35,\n",
    "    # n_rows=1,\n",
    "    legend_loc=\"bottom\",\n",
    "    xlim=(-175, 175),\n",
    "    ylim=(0, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section illustrates how to assess and compare the ranking effectiveness of\n",
    "classification models using Lift Charts, a valuable tool for evaluating how well \n",
    "a model prioritizes positive instances relative to random chance. Leveraging the \n",
    "Logistic Regression, Decision Tree, and Random Forest Classifier models trained \n",
    "on the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), \n",
    "we plot Lift curves to visualize their relative ability to surface high-value (positive) cases at the top of the prediction list.\n",
    "\n",
    "A Lift Chart plots the ratio of actual positives identified by the model compared \n",
    "to what would be expected by random selection, across increasingly larger proportions \n",
    "of the sample sorted by predicted probability. The baseline (Lift = 1) represents \n",
    "random chance; curves that rise above this line demonstrate the model's ability to\n",
    "\"lift\" positive outcomes toward the top ranks. This makes Lift Charts especially\n",
    "useful in applications like marketing, fraud detection, and risk stratification where\n",
    "targeting the top segment of predictions can yield outsized value. \n",
    "See the [mathematical definition of Lift here](https://lshpaner.github.io/model_metrics_docs/conceptual_notes.html#lift-mathematical-definition).\n",
    "\n",
    "The `show_lift_chart` function enables flexible creation of Lift Charts for one or more \n",
    "models. It supports single-plot overlays, subplot layouts, and detailed customization of \n",
    "labels, titles, and styling. Designed for both exploratory analysis and stakeholder \n",
    "presentation, this utility helps users better understand model ranking performance \n",
    "across the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Chart Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Lift Chart example, we evaluate and compare the ranking performance of two classification models: Logistic Regression and Random Forest Classifier trained on the synthetic dataset from the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification). The chart displays Lift curves for both models in a two-column subplot layout (`n_cols=2, n_rows=1`), enabling side-by-side comparison of how effectively each model prioritizes positive cases.\n",
    "\n",
    "Each plot shows the model’s Lift across increasing portions of the test set, with a grey dashed line at Lift = 1 indicating the baseline (random performance). Curves above this line reflect the model’s ability to identify more positives than would be expected by chance. The Random Forest produces a steeper initial lift, demonstrating greater concentration of positive cases near the top-ranked predictions.\n",
    "\n",
    "The `show_lift_chart` function allows for rich customization, including plot dimensions, axis font sizes, and curve styling. In this example, we set the line widths for both models and saved the plots in both PNG and SVG formats for further reporting or documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_lift_chart\n",
    "\n",
    "# Plot Lift chart\n",
    "show_lift_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    overlay=False,\n",
    "    model_title=model_titles,\n",
    "    save_plot=True,\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    subplots=True,\n",
    "    figsize=(12, 6),\n",
    "    label_fontsize=16,\n",
    "    tick_fontsize=14,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift Chart Example 2: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example overlays Lift curves from two classification models: Logistic Regression and Random Forest Classifier on a single plot for direct visual comparison. Both models were trained on the same synthetic dataset from the Binary Classification Models section, and their lift performance is evaluated on the shared test set.\n",
    "\n",
    "The Lift curve shows how many more positive outcomes are captured by the model at each quantile compared to a random baseline. A horizontal dashed black line at Lift = 1 represents random selection; curves above this line indicate effective ranking of positive cases. Overlaying curves makes it easier to assess which model better concentrates true positives near the top of the prediction list.\n",
    "\n",
    "Using the `overlay=True` option, the `show_lift_chart` function generates a clean, unified plot. Each curve is styled with `linewidth=2` for clarity, and all axis elements and tick marks are sized for presentation-quality output. This layout is particularly helpful for slide decks, performance reports, or model selection discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_lift_chart\n",
    "\n",
    "# Plot Lift chart\n",
    "show_lift_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    overlay=True,\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    figsize=(14, 10),\n",
    "    label_fontsize=16,\n",
    "    tick_fontsize=14,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain Charts\n",
    "\n",
    "This section explores how to evaluate the **cumulative performance** of classification models in identifying positive outcomes using **gain charts**. These charts are especially effective at showing the model’s ability to concentrate the correct (positive) predictions in the top-ranked portion of the dataset. Using the same Logistic Regression, Decision Tree, and Random Forest Classifier models trained on the synthetic dataset introduced in the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), we demonstrate how to plot and compare Gain Curves across models.\n",
    "\n",
    "\n",
    "A **gain chart** shows the cumulative percentage of actual positive cases captured\n",
    "as we move through the population sorted by predicted probability. Unlike the Lift Chart,\n",
    "which displays the ratio of model performance over baseline, the Gain Chart directly shows\n",
    "the percentage of positives captured, providing a more intuitive sense of how effective a model is\n",
    "at identifying positives early in the ranked list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Chart Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Gain Chart example, we compare the cumulative gain performance of two classification models:\n",
    "Logistic Regression and Random Forest Classifier. This visualization showcases their ability to identify positive instances across different percentiles\n",
    "of the ranked test data.\n",
    "\n",
    "Each subplot presents the **cumulative gain** achieved as a function of the percentage of the sample, sorted \n",
    "by descending predicted probability. The grey dashed line represents the **baseline (random gain)**. A model \n",
    "that identifies a high proportion of positive cases in the early part of the ranking will have a steeper and \n",
    "higher curve. In this example, the Random Forest model outpaces Logistic Regression, indicating \n",
    "better early identification of positives.\n",
    "\n",
    "The `show_gain_chart` function allows flexible styling and layout control. This example uses a subplot \n",
    "configuration (`n_cols=2, n_rows=1`), customized line widths and colors, and includes saving the figure \n",
    "for documentation or stakeholder presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_gain_chart\n",
    "\n",
    "show_gain_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    figsize=(12, 6),\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    save_plot=True,\n",
    "    subplots=True,\n",
    "    show_gini=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Chart Example 2: Displaying Gini Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to include Gini coefficients directly in the gain chart legends using \n",
    "the `show_gini=True` parameter. The Gini coefficient is a summary statistic derived from the area \n",
    "under the gain curve (AUGC), calculated as 2 × AUGC - 1, and ranges from 0 to 1 where higher values \n",
    "indicate better model discrimination.\n",
    "\n",
    "Both models: Logistic Regression and Random Forest Classifier were trained on the synthetic data from the \n",
    "[Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification). \n",
    "By enabling `show_gini=True` (and optionally setting `decimal_places=3`), each model's legend \n",
    "entry automatically displays its Gini coefficient, providing both visual and quantitative performance \n",
    "comparison in a single view.\n",
    "\n",
    "The Gini coefficient complements the visual gain curve by offering a single number that summarizes \n",
    "discriminative power. In this example, both the curve shape and the Gini value help identify which \n",
    "model better concentrates positive cases at the top of the predicted ranking. This is particularly \n",
    "useful in presentations, model selection discussions, and performance reporting where stakeholders \n",
    "need both graphical intuition and numeric metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_gain_chart\n",
    "\n",
    "show_gain_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    subplots=True,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    show_gini=True,\n",
    "    decimal_places=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Chart Example 3: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example overlays Gain curves from two classification models: Logistic\n",
    "Regression and Random Forest Classifier on a single plot to enable direct\n",
    "visual comparison of their cumulative gain performance. \n",
    "\n",
    "The Gain curve shows the cumulative proportion of true positives captured as \n",
    "you move through the population, ranked by predicted probability. A diagonal \n",
    "baseline line from (0, 0) to (1, 1) indicates the expected performance of a \n",
    "random model. Curves that rise above this line demonstrate superior model \n",
    "ability to concentrate positive cases near the top of the ranked list.\n",
    "\n",
    "By setting `overlay=True`, the `show_gain_chart` function produces a single, \n",
    "easy-to-read plot containing both models' gain curves. Each curve is styled \n",
    "with `linewidth=2` for clear visibility. Overlay layouts are ideal for model \n",
    "selection discussions, presentations, and performance dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_gain_chart\n",
    "\n",
    "show_gain_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    overlay=True,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_roc_curve` function provides flexible and highly customizable plotting of ROC curves for binary classification models. It supports overlays, subplot layouts, and subgroup visualizations, while also allowing export options and styling hooks for publication-ready output.\n",
    "\n",
    "Using the Logistic Regression and \n",
    "Random Forest Classifier models trained on the synthetic dataset introduced in the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), we generate ROC curves to visualize their discriminatory power.\n",
    "\n",
    "ROC AUC (Receiver Operating Characteristic Area Under the Curve) provides a \n",
    "single scalar value representing a model's ability to distinguish between \n",
    "positive and negative classes, with a value of 1 indicating perfect classification \n",
    "and 0.5 representing random guessing. The curves are plotted by varying the \n",
    "classification threshold and calculating the true positive rate (sensitivity) \n",
    "against the false positive rate (1-specificity). This makes ROC AUC particularly \n",
    "useful for comparing models like Logistic Regression, which relies on linear \n",
    "decision boundaries, and Random Forest Classifier, which leverages ensemble \n",
    "decision trees, especially when class imbalances or threshold sensitivity are \n",
    "concerns. The `show_roc_curve` function simplifies this process, enabling \n",
    "users to visualize and compare these curves effectively, setting the stage for \n",
    "detailed performance analysis in subsequent examples.\n",
    "\n",
    "The `show_roc_curve` function provides a flexible and powerful way to visualize \n",
    "the performance of binary classification models using Receiver Operating Characteristic \n",
    "(ROC) curves. Whether you're comparing multiple models, evaluating subgroup fairness, \n",
    "or preparing publication-ready plots, this function allows full control over layout,\n",
    "styling, and annotations. It supports single and multiple model inputs, optional overlay \n",
    "or subplot layouts, and group-wise comparisons via a categorical feature. Additional options \n",
    "allow custom axis labels, AUC precision, curve styling, and export to PNG/SVG. \n",
    "Designed to be both user-friendly and highly configurable, `show_roc_curve` \n",
    "is a practical tool for model evaluation and stakeholder communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first ROC AUC evaluation example, we plot the ROC curves for two \n",
    "models: Logistic Regression and Random Forest Classifier. The curves are displayed side by side \n",
    "using a subplot layout (`n_cols=2, n_rows=1`), with the Logistic Regression curve \n",
    "in blue and the Random Forest curve in green for clear differentiation. \n",
    "A red dashed line represents the random guessing baseline. This example \n",
    "demonstrates how the `show_roc_curve` function enables straightforward \n",
    "visualization of model performance, with options to customize colors, \n",
    "add a grid, and save the plot for reporting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 2: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second ROC AUC evaluation example, we focus on overlaying the results of \n",
    "two models: Logistic Regression and Random Forest Classifier onto a single plot. Using the `show_roc_curve` function with the `overlay=True` parameter, the ROC curves for both models are \n",
    "displayed together, with Logistic Regression in blue and Random Forest in black, \n",
    "both with a `linewidth=2`. A red dashed line serves as the random guessing \n",
    "baseline, and the plot includes a custom title for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    title=\"ROC Curves: Logistic Regression and Random Forest\",\n",
    "    overlay=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 3: DeLong's Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third ROC AUC evaluation example, we demonstrate how to statistically\n",
    "compare the performance of two correlated models using Hanley & McNeil's\n",
    "parametric AUC comparison (an approximation of DeLong's test). We utilize the\n",
    "Logistic Regression and Random Forest Classifier models. By passing their \n",
    "predicted probabilities to the `delong` parameter of the `show_roc_curve` function, we can assess whether the difference in AUC between the two models is statistically significant. This is particularly useful when models are evaluated on the same dataset, as it accounts for the inherent correlation in their predictions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    decimal_places=2,\n",
    "    delong=[model1.predict_proba(X_test)[:, 1], model2.predict_proba(X_test)[:, 1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 4: Hanley Mcneil AUC Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import hanley_mcneil_auc_test\n",
    "\n",
    "# Compare two models' ROC-AUC scores\n",
    "hanley_mcneil_auc_test(\n",
    "    y_test,\n",
    "    model1.predict_proba(X_test)[:, 1],\n",
    "    model2.predict_proba(X_test)[:, 1],\n",
    "    model_names=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    verbose=True,\n",
    "    decimal_places=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 5: Operating Point Using Youden's J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this fifth ROC AUC evaluation example, we again use the well-known\n",
    "*Adult Income* dataset, a widely adopted benchmark for binary classification.\n",
    "Its combination of categorical and numerical predictors makes it well suited for\n",
    "both performance evaluation and interpretability analyses.\n",
    "\n",
    "To train and evaluate the model, we rely on the `model_tuner` library.\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html#adult-income-training).\n",
    "\n",
    "The objective of this example is to **identify and visualize an optimal operating point**\n",
    "on the ROC curve using **Youden's J statistic**, defined as:\n",
    "\n",
    "$$\n",
    "   J = \\text{TPR} - \\text{FPR}\n",
    "$$\n",
    "\n",
    "This criterion selects the threshold that maximizes the vertical distance between\n",
    "the ROC curve and the random-guess diagonal, providing a balanced tradeoff between\n",
    "sensitivity and specificity.\n",
    "\n",
    "The `show_roc_curve` function supports this directly via the\n",
    "`show_operating_point` and `operating_point_method` parameters.\n",
    "\n",
    "In the example below, we compute the ROC curve for a decision tree classifier\n",
    "and annotate the optimal operating point determined by Youden's J statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    model_title=model_titles,\n",
    "    decimal_places=2,\n",
    "    show_operating_point=True,\n",
    "    subplots=True,\n",
    "    operating_point_method=\"youden\",\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    operating_point_kwgs={\n",
    "        \"marker\": \"o\",\n",
    "        \"color\": \"red\",\n",
    "        \"s\": 100,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 6: Closest to Top Left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we demonstrate an alternative method for identifying an optimal operating point\n",
    "on the ROC curve using the **closest-to-top-left** criterion. Like Youden's J statistic, this\n",
    "approach seeks a balanced threshold, but instead of maximizing the vertical distance from the\n",
    "diagonal, it minimizes the Euclidean distance to the ideal point (0, 1) in ROC space.\n",
    "\n",
    "The closest-to-top-left method finds the threshold that minimizes:\n",
    "\n",
    "$$\n",
    "   d = \\sqrt{(1 - \\text{TPR})^2 + \\text{FPR}^2}\n",
    "$$\n",
    "\n",
    "This geometric criterion is particularly useful when you want to prioritize proximity to perfect\n",
    "classification (top-left corner) rather than maximizing the difference between true positive\n",
    "and false positive rates.\n",
    "\n",
    "In this ROC AUC evaluation example, we focus on the results of \n",
    "two models: Logistic Regression and Random Forest Classifier, trained on the synthetic dataset from the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification).\n",
    "\n",
    "The ``show_roc_curve`` function supports this method through the ``operating_point_method`` parameter\n",
    "by setting it to ``\"closest_topleft\"``. In the example below, we compute the ROC curve for a\n",
    "decision tree classifier and annotate the optimal operating point using the closest-to-top-left\n",
    "criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    model_title=model_titles,\n",
    "    decimal_places=2,\n",
    "    show_operating_point=True,\n",
    "    subplots=True,\n",
    "    operating_point_method=\"closest_topleft\",\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    operating_point_kwgs={\n",
    "        \"marker\": \"o\",\n",
    "        \"color\": \"red\",\n",
    "        \"s\": 100,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 7: by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seventh ROC AUC evaluation example, we utilize the well-known \n",
    "*Adult Income* dataset, a widely used benchmark for binary classification \n",
    "tasks. Its rich mix of categorical and numerical features makes it particularly \n",
    "suitable for analyzing model performance across different subgroups.\n",
    "\n",
    "To build and evaluate our models, we use the `model_tuner` library. \n",
    "\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html).\n",
    "\n",
    "The objective here is to assess ROC AUC scores not just overall, but \n",
    "**across each category of a selected feature**, such as *occupation*, \n",
    "*education*, *marital-status*, or *race*. This approach enables deeper insight into how \n",
    "performance varies by subgroup, which is particularly important for fairness, \n",
    "bias detection, and subgroup-level interpretability.\n",
    "\n",
    "The `show_roc_curve` function supports this analysis through the \n",
    "`group_category` parameter. \n",
    "\n",
    "For example, by passing `group_category=X_test_2[\"race\"]`, \n",
    "you can generate a separate ROC curve for each unique racial group in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=\"Random Forest Classifier\",\n",
    "    decimal_places=2,\n",
    "    group_category=X_test_2_ai[\"race\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recal Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates how to evaluate the performance of binary classification \n",
    "models using Precision-Recall (PR) curves, a critical visualization for understanding \n",
    "model behavior in the presence of class imbalance. Using the Logistic Regression \n",
    "and Random Forest Classifier models trained on the \n",
    "synthetic dataset from the previous [(Binary Classification Models section)](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), \n",
    "we generate PR curves to examine how well each model identifies true positives while limiting false positives.\n",
    "\n",
    "Precision-Recall curves focus on the trade-off between **precision** \n",
    "(positive predictive value) and **recall** (sensitivity) across different \n",
    "classification thresholds. This is particularly important when the positive \n",
    "class is rare, as is common in fraud detection, disease diagnosis, or adverse \n",
    "event prediction, because ROC AUC can overstate performance under imbalance. \n",
    "Unlike the ROC curve, the PR curve is sensitive to the proportion of positive \n",
    "examples and gives a clearer picture of how well a model performs where it \n",
    "matters most: in identifying the positive class.\n",
    "\n",
    "The **area under the Precision-Recall curve**, also known as Average Precision \n",
    "(AP), summarizes model performance across thresholds. A model that maintains high \n",
    "precision as recall increases is generally more desirable, especially in settings \n",
    "where false positives have a high cost. This makes the PR curve a complementary \n",
    "and sometimes more informative tool than ROC AUC in skewed classification scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Precision-Recall evaluation example, we plot the PR curves for two \n",
    "models: Logistic Regression and Random Forest Classifier.\n",
    "\n",
    "The curves are arranged side by side using a subplot layout (``n_cols=2, n_rows=1``), \n",
    "with the Logistic Regression curve rendered in blue and the Random Forest curve \n",
    "in green to distinguish between models. A gray dashed line indicates the baseline \n",
    "precision, equal to the prevalence of the positive class in the dataset.\n",
    "\n",
    "This example illustrates how the ``show_pr_curve`` function makes it easy to \n",
    "visualize and compare model performance when dealing with class imbalance. It \n",
    "also demonstrates layout flexibility and customization options, including gridlines, \n",
    "label styling, and export functionality, making it suitable for both exploratory \n",
    "analysis and final reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_pr_curve\n",
    "\n",
    "show_pr_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Example 2: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second Precision-Recall evaluation example, we focus on overlaying the \n",
    "results of two models: Logistic Regression and Random Forest Classifier onto a single plot. \n",
    "Using the `show_pr_curve` function with the `overlay=True` parameter, the Precision-Recall curves for \n",
    "both models are displayed together, with Logistic Regression in blue and Random \n",
    "Forest in black, both with a `linewidth=2`. The plot includes a custom title \n",
    "for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_pr_curve\n",
    "\n",
    "show_pr_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    title=\"ROC Curves: Logistic Regression and Random Forest\",\n",
    "    overlay=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Example 3: Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third Precision-Recall evaluation example, we utilize the well-known \n",
    "*Adult Income* dataset, a widely used benchmark for binary classification \n",
    "tasks. Its rich mix of categorical and numerical features makes it particularly \n",
    "suitable for analyzing model performance across different subgroups.\n",
    "\n",
    "To build and evaluate our models, we use the `model_tuner` library. \n",
    "\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html).\n",
    "\n",
    "\n",
    "The objective here is to assess ROC AUC scores not just overall, but \n",
    "**across each category of a selected feature**, such as *occupation*, \n",
    "*education*, *marital-status*, or *race*. This approach enables deeper insight into how \n",
    "performance varies by subgroup, which is particularly important for fairness, \n",
    "bias detection, and subgroup-level interpretability.\n",
    "\n",
    "The `show_pr_curve` function supports this analysis through the \n",
    "`group_category` parameter. \n",
    "\n",
    "For example, by passing `group_category=X_test_2[\"race\"]`, \n",
    "you can generate a separate ROC curve for each unique racial group in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_pr_curve\n",
    "\n",
    "show_pr_curve(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=\"Random Forest Classifier\",\n",
    "    decimal_places=2,\n",
    "    group_category=X_test_2_ai[\"race\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces the `show_confusion_matrix` function, which provides a \n",
    "flexible, styled interface for generating and visualizing confusion matrices \n",
    "across one or more classification models. It supports advanced features like \n",
    "threshold overrides, subgroup labeling, classification report display, and fully \n",
    "customizable plot aesthetics including subplot layouts.\n",
    "\n",
    "The confusion matrix is a fundamental diagnostic tool for classification models, \n",
    "displaying the counts of true positives, true negatives, false positives, and \n",
    "false negatives. This function goes beyond standard implementations by allowing \n",
    "for custom thresholds (globally or per model), label annotation (e.g., TP, FP, etc.), \n",
    "plot exporting, colorbar toggling, and subplot visualization.\n",
    "\n",
    "This is especially useful when comparing multiple models side-by-side or needing \n",
    "publication-ready confusion matrices for stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example 1: Threshold=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first confusion matrix evaluation example, we focus on showing the\n",
    "results of two models: Logistic Regression and Random Forest Classifier, trained\n",
    "on the synthetic dataset from the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification) onto a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_confusion_matrix\n",
    "\n",
    "show_confusion_matrix(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    # cmap=\"viridis\",\n",
    "    text_wrap=40,\n",
    "    # title=\"Custom\",\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    figsize=(6, 6),\n",
    "    show_colorbar=False,\n",
    "    label_fontsize=16,\n",
    "    tick_fontsize=12,\n",
    "    inner_fontsize=14,\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example 2: Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_confusion_matrix\n",
    "\n",
    "show_confusion_matrix(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    cmap=\"viridis\",\n",
    "    text_wrap=40,\n",
    "    subplots=True,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    # figsize=(6, 6),\n",
    "    tick_fontsize=14,\n",
    "    inner_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    show_colorbar=True,\n",
    "    class_report=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example 3: Threshold = 0.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third confusion matrix evaluation example using the synthetic dataset \n",
    "from the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), we apply \n",
    "a custom classification threshold of 0.37 using the `custom_threshold` parameter. \n",
    "This overrides the default threshold of 0.5 and enables us to inspect how the \n",
    "confusion matrices shift when a more lenient decision boundary is applied. Refer \n",
    "to the section on [threshold selection logic](https://lshpaner.github.io/model_metrics_docs/conceptual_notes.html#threshold-selection-logic)\n",
    "for caveats on choosing the right threshold.\n",
    "\n",
    "This is especially useful in imbalanced classification problems or cost-sensitive \n",
    "environments where the trade-off between precision and recall must be adjusted. \n",
    "By lowering the threshold, we increase the number of positive predictions, \n",
    "which can improve recall but may come at the cost of more false positives.\n",
    "\n",
    "The output matrices for both models: Logistic Regression and Random Forest are shown \n",
    "side by side in a subplot layout for easy visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_confusion_matrix\n",
    "\n",
    "show_confusion_matrix(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    text_wrap=40,\n",
    "    subplots=True,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    figsize=(6, 6),\n",
    "    tick_fontsize=14,\n",
    "    inner_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    custom_threshold=0.37,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on calibration curves, a diagnostic tool that compares \n",
    "predicted probabilities to actual outcomes, helping evaluate how well a model's \n",
    "predicted confidence aligns with observed frequencies. Using models like Logistic \n",
    "Regression or Random Forest on the synthetic dataset from the previous \n",
    "[(Binary Classification Models)](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification) section, we generate \n",
    "calibration curves to assess the reliability of model probabilities.\n",
    "\n",
    "Calibration is especially important in domains where probability outputs inform \n",
    "downstream decisions, such as healthcare, finance, and risk management. A \n",
    "well-calibrated model not only predicts the correct class but also outputs \n",
    "meaningful probabilities, for example, when a model predicts a 0.7 probability, \n",
    "we expect roughly 70% of such predictions to be correct.\n",
    "\n",
    "The `show_calibration_curve` function simplifies this process by allowing users to \n",
    "visualize calibration performance across models or subgroups. The plots show the \n",
    "mean predicted probabilities against the actual observed fractions of positive \n",
    "cases, with an optional reference line representing perfect calibration. \n",
    "Additional features include support for overlay or subplot layouts, subgroup \n",
    "analysis by categorical features, and optional Brier score display, a scalar \n",
    "measure of calibration quality.\n",
    "\n",
    "The function offers full control over styling, figure layout, axis labels, and \n",
    "output format, making it easy to generate both exploratory and publication-ready \n",
    "plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curve Example 1: Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_calibration_curve\n",
    "\n",
    "show_calibration_curve(\n",
    "    model=pipelines_or_models[:2],\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=model_titles[:2],\n",
    "    text_wrap=50,\n",
    "    bins=10,\n",
    "    show_brier_score=True,\n",
    "    figsize=(12, 6),\n",
    "    subplots=True,\n",
    "    linestyle_kwgs={\"color\": \"black\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curve Example 2: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example also uses the well-known *Adult Income* dataset, a widely used \n",
    "benchmark for binary classification tasks. Its rich mix of categorical and \n",
    "numerical features makes it particularly suitable for analyzing model performance \n",
    "across different subgroups.\n",
    "\n",
    "To train and evaluate the model, we rely on the `model_tuner` library.\n",
    "\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html#adult-income-training).\n",
    "\n",
    "This example demonstrates how to overlay calibration curves from multiple classification \n",
    "models in a single plot. Overlaying allows for direct visual comparison of how predicted \n",
    "probabilities from each model align with actual outcomes on the same axes.\n",
    "\n",
    "The diagonal dashed line represents perfect calibration, and Brier scores are included \n",
    "in the legend for each model, providing a quantitative measure of calibration accuracy.\n",
    "\n",
    "By setting `overlay=True`, the function combines all model curves into one figure, \n",
    "making it easier to evaluate relative performance without splitting across subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_calibration_curve\n",
    "\n",
    "show_calibration_curve(\n",
    "    model=pipelines_or_models,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=model_titles_ai,\n",
    "    bins=10,\n",
    "    figsize=(14, 10),\n",
    "    show_brier_score=True,\n",
    "    overlay=True,\n",
    "    brier_decimals=4,\n",
    "    linestyle_kwgs={\"color\": \"black\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curve Example 3: by Category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_calibration_curve\n",
    "\n",
    "show_calibration_curve(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=\"Random Forest Classifier\",\n",
    "    bins=10,\n",
    "    show_brier_score=True,\n",
    "    brier_decimals=4,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    linestyle_kwgs={\"color\": \"black\"},\n",
    "    curve_kwgs={title: {\"linewidth\": 2} for title in model_titles_ai},\n",
    "    group_category=X_test_2_ai[\"race\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Metric Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces a powerful utility for exploring how classification \n",
    "thresholds affect key performance metrics, including **Precision**, **Recall**, \n",
    "**F1 Score**, and **Specificity**. Rather than fixing a threshold (commonly at 0.5),\n",
    "this function allows users to visualize **trade-offs across the full range of \n",
    "possible thresholds**, making it especially useful when optimizing for use-case-specific \n",
    "goals such as maximizing recall or achieving a minimum precision.\n",
    "\n",
    "Using the Random Forest Classifier models trained on the \n",
    "[adult income dataset](https://lshpaner.github.io/model_metrics_docs/model_training.html), \n",
    "this tool helps users answer practical questions like:\n",
    "\n",
    "- *What threshold achieves at least 85% precision?*\n",
    "- *Where does F1 score peak for this model?*\n",
    "- *How does specificity behave as the threshold increases?*\n",
    "\n",
    "The plot_threshold_metrics function supports optional threshold lookups via \n",
    "`lookup_metric` and `lookup_value`, which prints the closest threshold that\n",
    "meets your constraint. Plots can be customized with colors, gridlines, line styles,\n",
    "wrapped titles, and export options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Curves Example 1: Threshold=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to plot threshold-dependent classification metrics .\n",
    "\n",
    "The `plot_threshold_metrics` function visualizes how Precision, Recall, F1 Score, \n",
    "and Specificity change as the decision threshold varies. In this configuration, \n",
    "the baseline threshold line at 0.5 is enabled (`baseline_thresh=True`), \n",
    "and the line styling is customized via `curve_kwgs`. Font sizes and wrapping options \n",
    "are adjusted for improved clarity in presentation-ready plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import plot_threshold_metrics\n",
    "\n",
    "plot_threshold_metrics(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X_test=X_test_ai,\n",
    "    y_test=y_test_ai,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    baseline_thresh=True,\n",
    "    baseline_kwgs={\n",
    "        \"color\": \"purple\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    curve_kwgs={\n",
    "        \"linestyle\": \"-\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    text_wrap=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Curves Example 2: Targeted Metric Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example expands on threshold-based classification metric visualization using \n",
    "a targeted lookup scenario. Suppose a clinical stakeholder or domain expert has \n",
    "determined (based on prior research, cost-benefit considerations, or operational\n",
    "constraints) that a precision of approximately `0.879` is ideal for downstream \n",
    "decision-making (e.g., minimizing false positives in a healthcare setting).\n",
    "\n",
    "The `plot_threshold_metrics` function accepts the optional arguments `lookup_metric` \n",
    "and `lookup_value` to help identify the threshold that best aligns with this target. \n",
    "When these are set, the function automatically locates and highlights the threshold \n",
    "that most closely achieves the desired metric value, offering transparency and \n",
    "guidance for threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import plot_threshold_metrics\n",
    "\n",
    "plot_threshold_metrics(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X_test=X_test_ai,\n",
    "    y_test=y_test_ai,\n",
    "    lookup_metric=\"precision\",\n",
    "    lookup_value=0.879,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    baseline_thresh=False,\n",
    "    lookup_kwgs={\n",
    "        \"color\": \"red\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    curve_kwgs={\n",
    "        \"linestyle\": \"-\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    text_wrap=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Curves Example 3: Model-Specific Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many production settings, a classifier is deployed with a tuned decision threshold different from the default 0.5 (e.g., to balance costs of false positives vs. false negatives).\n",
    "This example shows how to **explicitly pass a model's chosen threshold** to be drawn as a vertical guide on the plot using `model_threshold=....`\n",
    "You can do this whether you're providing a model/X pair or pre-computed probabilities via `y_prob`. Below we show the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for Random Forest model\n",
    "y_prob_rf = model_rf[\"model\"].estimator.predict_proba(X_test_ai)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model thresholds\n",
    "model_thresholds = {\n",
    "    \"Logistic Regression\": next(iter(model_lr[\"model\"].threshold.values())),\n",
    "    \"Decision Tree Classifier\": next(iter(model_dt[\"model\"].threshold.values())),\n",
    "    \"Random Forest Classifier\": next(iter(model_rf[\"model\"].threshold.values())),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import plot_threshold_metrics\n",
    "\n",
    "# Example: Use precomputed probabilities but still highlight the model's tuned threshold.\n",
    "plot_threshold_metrics(\n",
    "    y_prob=y_prob_rf,  # precomputed probabilities for the positive class\n",
    "    y_test=y_test_ai,  # ground-truth labels\n",
    "    baseline_thresh=False,  # hide the default 0.5 guide\n",
    "    model_threshold=model_thresholds[\"Random Forest Classifier\"],\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    threshold_kwgs={  # styling for the model-threshold vertical line\n",
    "        \"color\": \"blue\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    curve_kwgs={  # styling for metric curves\n",
    "        \"linestyle\": \"-\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    text_wrap=40,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
