{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import model_metrics\n",
    "\n",
    "print(model_metrics.__version__)\n",
    "\n",
    "from model_tuner import loadObjects\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from eda_toolkit import ensure_directory\n",
    "from model_metrics import (\n",
    "    summarize_model_performance,\n",
    "    show_calibration_curve,\n",
    "    show_confusion_matrix,\n",
    "    show_roc_curve,\n",
    "    show_pr_curve,\n",
    "    show_lift_chart,\n",
    "    show_gain_chart,\n",
    "    plot_threshold_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Metrics version: {model_metrics.__version__}\")\n",
    "print(f\"Model Metrics authors: {model_metrics.__author__} \\n\")\n",
    "\n",
    "## Define base paths\n",
    "## `base_path`` represents the parent directory of current working directory\n",
    "base_path = os.path.join(os.pardir)\n",
    "## Go up one level from 'notebooks' to the parent directory, then into the\n",
    "## 'results' folder\n",
    "\n",
    "model_path = os.path.join(os.pardir, \"model_files/results\")\n",
    "data_path = os.path.join(os.pardir, \"model_files\")\n",
    "image_path_png = os.path.join(data_path, \"images\", \"png_images\")\n",
    "image_path_svg = os.path.join(data_path, \"images\", \"svg_images\")\n",
    "\n",
    "# Use the function to ensure the 'data' directory exists\n",
    "ensure_directory(model_path)\n",
    "ensure_directory(image_path_png)\n",
    "ensure_directory(image_path_svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a synthetic dataset for classification using `make_classification` from `sklearn.datasets`.\n",
    "This dataset will be used to train and evaluate multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train models\n",
    "model1 = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
    "model2 = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "model_titles = [\"Logistic Regression\", \"Random Forest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Predictions from Adult Income Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X_ai = adult.data.features\n",
    "\n",
    "# X.to_csv(\"../data/X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = loadObjects(os.path.join(model_path, \"LogisticRegression.pkl\"))\n",
    "model_dt = loadObjects(os.path.join(model_path, \"DecisionTreeClassifier.pkl\"))\n",
    "model_rf = loadObjects(os.path.join(model_path, \"RandomForestClassifier.pkl\"))\n",
    "\n",
    "\n",
    "X_test_ai = pd.read_parquet(os.path.join(data_path, \"X_test.parquet\"))\n",
    "y_test_ai = pd.read_parquet(os.path.join(data_path, \"y_test.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set The Desired Naming Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_or_models = [\n",
    "    model_lr[\"model\"].estimator,\n",
    "    model_rf[\"model\"].estimator,\n",
    "    model_dt[\"model\"].estimator,\n",
    "]\n",
    "\n",
    "# Model titles\n",
    "model_titles_ai = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Random Forest Classifier\",\n",
    "    \"Decision Tree Classifier\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2_ai = X_test_ai.join(\n",
    "    X_ai[[\"sex\", \"race\", \"relationship\", \"occupation\", \"workclass\", \"education\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize model performance with default threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we summarize the performance of multiple models using the default threshold of 0.5 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "model_performance = summarize_model_performance(\n",
    "    model=[model1, model2],\n",
    "    model_title=model_titles,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_type=\"classification\",\n",
    "    return_df=True,\n",
    ")\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize model performance with custom threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we summarize the performance of multiple models using a custom threshold of 0.2 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "model_performance = summarize_model_performance(\n",
    "    model=[model1, model2],\n",
    "    model_title=model_titles,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_type=\"classification\",\n",
    "    return_df=True,\n",
    "    custom_threshold=0.2,\n",
    ")\n",
    "\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model Examples w/ Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells, we will demonstrate model performance summarization for regression models using the diabetes dataset from `sklearn.datasets`.\n",
    "This dataset will be used to train and evaluate multiple regression models, and we will summarize their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "diabetes = load_diabetes(as_frame=True)[\"frame\"]\n",
    "X_diabetes = diabetes.drop(columns=[\"target\"])\n",
    "y_diabetes = diabetes[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_diabetes, X_test_diabetes, y_train_diabetes, y_test_diabetes = train_test_split(\n",
    "    X_diabetes,\n",
    "    y_diabetes,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train Linear Regression (on unscaled data)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_diabetes, y_train_diabetes)\n",
    "# Train Random Forest Regressor (on unscaled data)\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    ")\n",
    "rf_model.fit(X_train_diabetes, y_train_diabetes)\n",
    "\n",
    "# Train Ridge Regression (on scaled data)\n",
    "ridge_model = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"estimator\", Ridge(alpha=1.0)),\n",
    "    ]\n",
    ")\n",
    "ridge_model.fit(X_train_diabetes, y_train_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example 1: Summarize Model Performance: Linear, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    return_df=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example 2: Summarize Model Performance: Linear, Ridge, RF (w/ Feature Importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Regression Example 2, we extend the analysis by introducing a Random Forest Regressor alongside Linear Regression and Ridge Regression to demonstrate how a model with feature importances, rather than coefficients, impacts evaluation outcomes. The code uses the `summarize_model_performance` function from `model_metrics` to assess all three models on the diabetes dataset’s test set, ensuring the Random Forest’s feature importance-based predictions are reflected in the results while preserving the coefficient-based results of the other models, as shown in the subsequent table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    return_df=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression example 3: Summarize Model Performance (Adjusted $R^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    return_df=True,\n",
    "    include_adjusted_r2=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression example 4 - Summarize Model Performance (Overall Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some scenarios, you may want to simplify the output by excluding variables, coefficients, and feature importances from the model results. This example demonstrates how to achieve that by setting `overall_only=True` in the `summarize_model_performance` function, producing a concise table that focuses on key metrics: model name, Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Explained Variance, and $R^2$ Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    overall_only=True,\n",
    "    return_df=True,\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression example 5: Printed Table with Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Regression Example 5, we illustrate how to generate a printed table that includes the Adjusted $R^2$ metric for regression models. By setting the `include_adjusted_r2` parameter to `True` in the `summarize_model_performance` function, we ensure that the output table provides a comprehensive view of model performance, including both $R^2$ and Adjusted $R^2$ values, which account for the number of predictors in the model. This allows for a more nuanced evaluation of model effectiveness, especially when comparing models with different numbers of features. Here, we toggle `return_df` to `False` (or simply do not pass it) to display the results directly as a printed table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import summarize_model_performance\n",
    "\n",
    "regression_metrics = summarize_model_performance(\n",
    "    model=[linear_model, ridge_model, rf_model],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    model_type=\"regression\",\n",
    "    decimal_places=2,\n",
    ")\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual diagnostics are essential tools for evaluating regression model performance beyond \n",
    "standard metrics like $R^2$ or RMSE. By examining the patterns in residuals: the differences \n",
    "between observed and predicted values, we can identify violations of modeling assumptions, \n",
    "detect systematic errors, and uncover opportunities for model improvement.\n",
    "\n",
    "The `show_residual_diagnostics` function provides comprehensive visualization of residual \n",
    "patterns across multiple dimensions:\n",
    "\n",
    "- **Residuals vs Fitted Values**: Assess homoscedasticity (constant variance) and identify non-linear patterns\n",
    "- **Residuals vs Predictors**: Examine whether specific features are associated with systematic prediction errors\n",
    "- **Q-Q Plots**: Evaluate whether residuals follow a normal distribution\n",
    "- **Histogram of Residuals**: Visualize the distribution shape and identify outliers\n",
    "- **Scale-Location Plots**: Detect heteroscedasticity (non-constant variance)\n",
    "\n",
    "**What Good Residuals Look Like:**\n",
    "\n",
    "- Randomly scattered around zero with no systematic patterns\n",
    "- Constant spread across the range of fitted values (homoscedasticity)\n",
    "- Approximately normally distributed (for inference and prediction intervals)\n",
    "- No strong correlations with individual predictor variables\n",
    "\n",
    "**What Bad Residuals Reveal:**\n",
    "\n",
    "- **Funnel shapes** (heteroscedasticity): Variance increases/decreases with predicted values, suggesting transformations may be needed\n",
    "- **Curved patterns**: Non-linear relationships that the model hasn't captured\n",
    "- **Clusters or groups**: Systematic differences across subpopulations that may require interaction terms or stratified models\n",
    "- **Heavy tails or skewness**: Outliers or violations of normality assumptions\n",
    "- **Patterns vs predictors**: Missing interaction effects or non-linear relationships with specific features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_prob = linear_model.predict(X_test_diabetes)\n",
    "rf_prob = rf_model.predict(X_test_diabetes)\n",
    "ridge_prob = ridge_model.predict(X_test_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_residual_diagnostics, resid_diagnostics_to_dataframe\n",
    "\n",
    "diagnostics = show_residual_diagnostics(\n",
    "    y_pred=rf_prob,\n",
    "    model_title=[\"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    n_clusters=3,\n",
    "    n_cols=2,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    tick_fontsize=12,\n",
    "    label_fontsize=14,\n",
    "    plot_type=\"histogram\",\n",
    "    show_centroids=True,\n",
    "    centroid_kwgs={\"c\": [\"red\", \"blue\", \"green\"], \"marker\": \"X\", \"s\": 50},\n",
    "    heteroskedasticity_test=\"all\",\n",
    "    legend_loc=\"upper right\",\n",
    "    show_diagnostics_table=True,\n",
    "    return_diagnostics=True,\n",
    "    show_plots=False,\n",
    "    decimal_places=2,\n",
    "    histogram_type=\"density\",\n",
    "    # figsize=(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resid_diagnostics_to_dataframe(diagnostics)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Diagnostics by Categorical Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particularly powerful extension of residual diagnostics is stratification by categorical \n",
    "variables such as demographic groups, treatment arms, or geographic regions. By examining \n",
    "residuals separately for each subgroup, we can identify whether:\n",
    "\n",
    "- Model performance is consistent across populations\n",
    "- Systematic bias exists for specific groups\n",
    "- Variance differs across subgroups (heteroscedasticity by group)\n",
    "- Interaction effects between predictors and group membership are present\n",
    "\n",
    "This is especially critical in applications where fairness and equity matter such as \n",
    "healthcare, lending, and social services, where models should not systematically  \n",
    "under-predict or over-predict for protected or vulnerable populations.\n",
    "\n",
    "This example demonstrates how to examine residual patterns across categorical subgroups \n",
    "using the `group_category` parameter. By stratifying residual diagnostics by a \n",
    "categorical variable: such as sex, age group, or treatment arm, we can identify whether \n",
    "model errors are consistent across subpopulations or if certain groups exhibit systematic \n",
    "bias or heteroscedasticity.\n",
    "\n",
    "In this example, we evaluate three regression models trained on the diabetes dataset: \n",
    "Linear Regression, Ridge Regression, and Random Forest. The `sex` variable in the \n",
    "original dataset is encoded numerically (positive/negative values), so we first transform \n",
    "it into interpretable categories (\"Male\" and \"Female\") before passing it to \n",
    "`show_residual_diagnostics`.\n",
    "\n",
    "The `plot_type=\"predictors\"` option generates residual plots for each predictor variable, \n",
    "with points color-coded by the categorical group. This allows us to visually assess whether:\n",
    "\n",
    "- Residuals are centered around zero for both groups\n",
    "- Variance is similar across groups (homoscedasticity)\n",
    "- Any systematic patterns exist that might indicate interaction effects or model misspecification\n",
    "\n",
    "When `show_centroids=True` is enabled, group centroids are overlaid on the plots to \n",
    "highlight the mean residual behavior for each subgroup. The `centroid_kwgs` parameter \n",
    "allows customization of these centroids with specific colors, markers, and sizes to \n",
    "distinguish between groups clearly.\n",
    "\n",
    "This type of analysis is particularly valuable in healthcare and social science applications \n",
    "where fairness and equity are critical concerns. Identifying residual patterns by demographic \n",
    "variables can reveal whether a model's predictions are systematically biased against \n",
    "specific subpopulations, informing decisions about model refinement or the need for \n",
    "group-specific calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'sex' column is already categorical-like (coded as positive/negative values)\n",
    "# Let's make it more interpretable\n",
    "X_test_diab_copy = X_test_diabetes.copy()\n",
    "X_test_diab_copy[\"sex_category\"] = X_test_diab_copy[\"sex\"].apply(\n",
    "    lambda x: \"Male\" if x > 0 else \"Female\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_diab_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_residual_diagnostics\n",
    "\n",
    "show_residual_diagnostics(\n",
    "    y_pred=[linear_prob, ridge_prob, rf_prob],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diab_copy[[\"age\", \"bmi\", \"sex_category\"]],\n",
    "    y=y_test_diabetes,\n",
    "    plot_type=\"predictors\",\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    show_centroids=True,\n",
    "    group_category=\"sex_category\",\n",
    "    centroid_kwgs={\"c\": [\"red\", \"blue\"], \"marker\": \"X\", \"s\": 50},\n",
    "    legend_loc=\"bottom\",\n",
    "    heteroskedasticity_test=\"breusch_pagan\",\n",
    "    figsize=(12, 8),\n",
    "    suptitle=\"\",\n",
    "    decimal_places=2,\n",
    "    kmeans_rstate=222\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_residual_diagnostics(\n",
    "    # model=[linear_model, ridge_model, rf_model],\n",
    "    y_pred=[linear_prob, ridge_prob, rf_prob],\n",
    "    model_title=[\"Linear Regression\", \"Ridge Regression\", \"Random Forest\"],\n",
    "    X=X_test_diabetes,\n",
    "    y=y_test_diabetes,\n",
    "    # plot_type=\"predictors\",\n",
    "    # show_lowess=True,\n",
    "    # line_kwgs={\"color\": \"black\", \"linestyle\": \"--\"},\n",
    "    # lowess_kwgs={\n",
    "    #     \"color\": \"orange\",\n",
    "    #     \"linewidth\": 3,\n",
    "    #     \"linestyle\": \"--\",\n",
    "    #     \"label\": \"Trend\",\n",
    "    # },\n",
    "    n_clusters=3,\n",
    "    show_centroids=True,\n",
    "    # centroid_kwgs={\"marker\": \"D\", \"s\": 2, \"color\": \"red\"},\n",
    "    centroid_kwgs={\"c\": [\"red\", \"blue\", \"green\"], \"marker\": \"X\", \"s\": 50},\n",
    "    heteroskedasticity_test=\"breusch_pagan\",\n",
    "    kmeans_rstate=222,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Lift chart\n",
    "show_lift_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    overlay=False,\n",
    "    model_title=model_titles,\n",
    "    save_plot=True,\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    subplots=True,\n",
    "    figsize=(12, 6),\n",
    "    label_fontsize=16,\n",
    "    tick_fontsize=14,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Lift chart\n",
    "show_lift_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    overlay=True,\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    figsize=(14, 10),\n",
    "    label_fontsize=16,\n",
    "    tick_fontsize=14,\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain Charts\n",
    "\n",
    "This section explores how to evaluate the **cumulative performance** of classification models in identifying positive outcomes using **gain charts**. These charts are especially effective at showing the model’s ability to concentrate the correct (positive) predictions in the top-ranked portion of the dataset. Using the same Logistic Regression, Decision Tree, and Random Forest Classifier models trained on the synthetic dataset introduced in the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), we demonstrate how to plot and compare Gain Curves across models.\n",
    "\n",
    "\n",
    "A **gain chart** shows the cumulative percentage of actual positive cases captured\n",
    "as we move through the population sorted by predicted probability. Unlike the Lift Chart,\n",
    "which displays the ratio of model performance over baseline, the Gain Chart directly shows\n",
    "the percentage of positives captured, providing a more intuitive sense of how effective a model is\n",
    "at identifying positives early in the ranked list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Chart Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Gain Chart example, we compare the cumulative gain performance of two classification models:\n",
    "Logistic Regression and Random Forest Classifier. This visualization showcases their ability to identify positive instances across different percentiles\n",
    "of the ranked test data.\n",
    "\n",
    "Each subplot presents the **cumulative gain** achieved as a function of the percentage of the sample, sorted \n",
    "by descending predicted probability. The grey dashed line represents the **baseline (random gain)**. A model \n",
    "that identifies a high proportion of positive cases in the early part of the ranking will have a steeper and \n",
    "higher curve. In this example, the Random Forest model outpaces Logistic Regression, indicating \n",
    "better early identification of positives.\n",
    "\n",
    "The `show_gain_chart` function allows flexible styling and layout control. This example uses a subplot \n",
    "configuration (`n_cols=2, n_rows=1`), customized line widths and colors, and includes saving the figure \n",
    "for documentation or stakeholder presentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Chart Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Gain Chart example, we compare the cumulative gain performance of two classification models:\n",
    "Logistic Regression and Random Forest Classifier. This visualization showcases their ability to identify positive instances across different percentiles\n",
    "of the ranked test data.\n",
    "\n",
    "Each subplot presents the **cumulative gain** achieved as a function of the percentage of the sample, sorted \n",
    "by descending predicted probability. The grey dashed line represents the **baseline (random gain)**. A model \n",
    "that identifies a high proportion of positive cases in the early part of the ranking will have a steeper and \n",
    "higher curve. In this example, the Random Forest model outpaces Logistic Regression, indicating \n",
    "better early identification of positives.\n",
    "\n",
    "The `show_gain_chart` function allows flexible styling and layout control. This example uses a subplot \n",
    "configuration (`n_cols=2, n_rows=1`), customized line widths and colors, and includes saving the figure \n",
    "for documentation or stakeholder presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_gain_chart\n",
    "\n",
    "show_gain_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    figsize=(12, 6),\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    save_plot=True,\n",
    "    subplots=True,\n",
    "    show_gini=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_gain_chart\n",
    "\n",
    "show_gain_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    subplots=True,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    show_gini=True,\n",
    "    decimal_places=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Chart Example 3: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example overlays Gain curves from two classification models: Logistic\n",
    "Regression and Random Forest Classifier on a single plot to enable direct\n",
    "visual comparison of their cumulative gain performance. \n",
    "\n",
    "The Gain curve shows the cumulative proportion of true positives captured as \n",
    "you move through the population, ranked by predicted probability. A diagonal \n",
    "baseline line from (0, 0) to (1, 1) indicates the expected performance of a \n",
    "random model. Curves that rise above this line demonstrate superior model \n",
    "ability to concentrate positive cases near the top of the ranked list.\n",
    "\n",
    "By setting `overlay=True`, the `show_gain_chart` function produces a single, \n",
    "easy-to-read plot containing both models' gain curves. Each curve is styled \n",
    "with `linewidth=2` for clear visibility. Overlay layouts are ideal for model \n",
    "selection discussions, presentations, and performance dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_gain_chart\n",
    "\n",
    "show_gain_chart(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    overlay=True,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    model_title=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\", \"linewidth\": 2},\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_roc_curve` function provides flexible and highly customizable plotting of ROC curves for binary classification models. It supports overlays, subplot layouts, and subgroup visualizations, while also allowing export options and styling hooks for publication-ready output.\n",
    "\n",
    "Using the Logistic Regression and \n",
    "Random Forest Classifier models trained on the synthetic dataset introduced in the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), we generate ROC curves to visualize their discriminatory power.\n",
    "\n",
    "ROC AUC (Receiver Operating Characteristic Area Under the Curve) provides a \n",
    "single scalar value representing a model's ability to distinguish between \n",
    "positive and negative classes, with a value of 1 indicating perfect classification \n",
    "and 0.5 representing random guessing. The curves are plotted by varying the \n",
    "classification threshold and calculating the true positive rate (sensitivity) \n",
    "against the false positive rate (1-specificity). This makes ROC AUC particularly \n",
    "useful for comparing models like Logistic Regression, which relies on linear \n",
    "decision boundaries, and Random Forest Classifier, which leverages ensemble \n",
    "decision trees, especially when class imbalances or threshold sensitivity are \n",
    "concerns. The `show_roc_curve` function simplifies this process, enabling \n",
    "users to visualize and compare these curves effectively, setting the stage for \n",
    "detailed performance analysis in subsequent examples.\n",
    "\n",
    "The `show_roc_curve` function provides a flexible and powerful way to visualize \n",
    "the performance of binary classification models using Receiver Operating Characteristic \n",
    "(ROC) curves. Whether you're comparing multiple models, evaluating subgroup fairness, \n",
    "or preparing publication-ready plots, this function allows full control over layout,\n",
    "styling, and annotations. It supports single and multiple model inputs, optional overlay \n",
    "or subplot layouts, and group-wise comparisons via a categorical feature. Additional options \n",
    "allow custom axis labels, AUC precision, curve styling, and export to PNG/SVG. \n",
    "Designed to be both user-friendly and highly configurable, `show_roc_curve` \n",
    "is a practical tool for model evaluation and stakeholder communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first ROC AUC evaluation example, we plot the ROC curves for two \n",
    "models: Logistic Regression and Random Forest Classifier. The curves are displayed side by side \n",
    "using a subplot layout (`n_cols=2, n_rows=1`), with the Logistic Regression curve \n",
    "in blue and the Random Forest curve in green for clear differentiation. \n",
    "A red dashed line represents the random guessing baseline. This example \n",
    "demonstrates how the `show_roc_curve` function enables straightforward \n",
    "visualization of model performance, with options to customize colors, \n",
    "add a grid, and save the plot for reporting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 2: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second ROC AUC evaluation example, we focus on overlaying the results of \n",
    "two models: Logistic Regression and Random Forest Classifier onto a single plot. Using the `show_roc_curve` function with the `overlay=True` parameter, the ROC curves for both models are \n",
    "displayed together, with Logistic Regression in blue and Random Forest in black, \n",
    "both with a `linewidth=2`. A red dashed line serves as the random guessing \n",
    "baseline, and the plot includes a custom title for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    title=\"ROC Curves: Logistic Regression and Random Forest\",\n",
    "    overlay=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 3: DeLong's Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third ROC AUC evaluation example, we demonstrate how to statistically\n",
    "compare the performance of two correlated models using Hanley & McNeil's\n",
    "parametric AUC comparison (an approximation of DeLong's test). We utilize the\n",
    "Logistic Regression and Random Forest Classifier models. By passing their \n",
    "predicted probabilities to the `delong` parameter of the `show_roc_curve` function, we can assess whether the difference in AUC between the two models is statistically significant. This is particularly useful when models are evaluated on the same dataset, as it accounts for the inherent correlation in their predictions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    decimal_places=2,\n",
    "    delong=[model1.predict_proba(X_test)[:, 1], model2.predict_proba(X_test)[:, 1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 4: Hanley Mcneil AUC Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import hanley_mcneil_auc_test\n",
    "\n",
    "# Compare two models' ROC-AUC scores\n",
    "hanley_mcneil_auc_test(\n",
    "    y_test,\n",
    "    model1.predict_proba(X_test)[:, 1],\n",
    "    model2.predict_proba(X_test)[:, 1],\n",
    "    model_names=[\"Logistic Regression\", \"Random Forest\"],\n",
    "    verbose=True,\n",
    "    decimal_places=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 5: Operating Point Using Youden's J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this fifth ROC AUC evaluation example, we again use the well-known\n",
    "*Adult Income* dataset, a widely adopted benchmark for binary classification.\n",
    "Its combination of categorical and numerical predictors makes it well suited for\n",
    "both performance evaluation and interpretability analyses.\n",
    "\n",
    "To train and evaluate the model, we rely on the `model_tuner` library.\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html#adult-income-training).\n",
    "\n",
    "The objective of this example is to **identify and visualize an optimal operating point**\n",
    "on the ROC curve using **Youden's J statistic**, defined as:\n",
    "\n",
    "$$\n",
    "   J = \\text{TPR} - \\text{FPR}\n",
    "$$\n",
    "\n",
    "This criterion selects the threshold that maximizes the vertical distance between\n",
    "the ROC curve and the random-guess diagonal, providing a balanced tradeoff between\n",
    "sensitivity and specificity.\n",
    "\n",
    "The `show_roc_curve` function supports this directly via the\n",
    "`show_operating_point` and `operating_point_method` parameters.\n",
    "\n",
    "In the example below, we compute the ROC curve for a decision tree classifier\n",
    "and annotate the optimal operating point determined by Youden's J statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    model_title=model_titles,\n",
    "    decimal_places=2,\n",
    "    show_operating_point=True,\n",
    "    subplots=True,\n",
    "    operating_point_method=\"youden\",\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    operating_point_kwgs={\n",
    "        \"marker\": \"o\",\n",
    "        \"color\": \"red\",\n",
    "        \"s\": 100,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 6: Closest to Top Left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we demonstrate an alternative method for identifying an optimal operating point\n",
    "on the ROC curve using the **closest-to-top-left** criterion. Like Youden's J statistic, this\n",
    "approach seeks a balanced threshold, but instead of maximizing the vertical distance from the\n",
    "diagonal, it minimizes the Euclidean distance to the ideal point (0, 1) in ROC space.\n",
    "\n",
    "The closest-to-top-left method finds the threshold that minimizes:\n",
    "\n",
    "$$\n",
    "   d = \\sqrt{(1 - \\text{TPR})^2 + \\text{FPR}^2}\n",
    "$$\n",
    "\n",
    "This geometric criterion is particularly useful when you want to prioritize proximity to perfect\n",
    "classification (top-left corner) rather than maximizing the difference between true positive\n",
    "and false positive rates.\n",
    "\n",
    "In this ROC AUC evaluation example, we focus on the results of \n",
    "two models: Logistic Regression and Random Forest Classifier, trained on the synthetic dataset from the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification).\n",
    "\n",
    "The ``show_roc_curve`` function supports this method through the ``operating_point_method`` parameter\n",
    "by setting it to ``\"closest_topleft\"``. In the example below, we compute the ROC curve for a\n",
    "decision tree classifier and annotate the optimal operating point using the closest-to-top-left\n",
    "criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    model_title=model_titles,\n",
    "    decimal_places=2,\n",
    "    show_operating_point=True,\n",
    "    subplots=True,\n",
    "    operating_point_method=\"closest_topleft\",\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    linestyle_kwgs={\"color\": \"red\", \"linestyle\": \"--\"},\n",
    "    operating_point_kwgs={\n",
    "        \"marker\": \"o\",\n",
    "        \"color\": \"red\",\n",
    "        \"s\": 100,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Example 7: by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seventh ROC AUC evaluation example, we utilize the well-known \n",
    "*Adult Income* dataset, a widely used benchmark for binary classification \n",
    "tasks. Its rich mix of categorical and numerical features makes it particularly \n",
    "suitable for analyzing model performance across different subgroups.\n",
    "\n",
    "To build and evaluate our models, we use the `model_tuner` library. \n",
    "\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html).\n",
    "\n",
    "The objective here is to assess ROC AUC scores not just overall, but \n",
    "**across each category of a selected feature**, such as *occupation*, \n",
    "*education*, *marital-status*, or *race*. This approach enables deeper insight into how \n",
    "performance varies by subgroup, which is particularly important for fairness, \n",
    "bias detection, and subgroup-level interpretability.\n",
    "\n",
    "The `show_roc_curve` function supports this analysis through the \n",
    "`group_category` parameter. \n",
    "\n",
    "For example, by passing `group_category=X_test_2[\"race\"]`, \n",
    "you can generate a separate ROC curve for each unique racial group in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_roc_curve\n",
    "\n",
    "show_roc_curve(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=\"Random Forest Classifier\",\n",
    "    decimal_places=2,\n",
    "    group_category=X_test_2_ai[\"race\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recal Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates how to evaluate the performance of binary classification \n",
    "models using Precision-Recall (PR) curves, a critical visualization for understanding \n",
    "model behavior in the presence of class imbalance. Using the Logistic Regression \n",
    "and Random Forest Classifier models trained on the \n",
    "synthetic dataset from the previous [(Binary Classification Models section)](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), \n",
    "we generate PR curves to examine how well each model identifies true positives while limiting false positives.\n",
    "\n",
    "Precision-Recall curves focus on the trade-off between **precision** \n",
    "(positive predictive value) and **recall** (sensitivity) across different \n",
    "classification thresholds. This is particularly important when the positive \n",
    "class is rare, as is common in fraud detection, disease diagnosis, or adverse \n",
    "event prediction, because ROC AUC can overstate performance under imbalance. \n",
    "Unlike the ROC curve, the PR curve is sensitive to the proportion of positive \n",
    "examples and gives a clearer picture of how well a model performs where it \n",
    "matters most: in identifying the positive class.\n",
    "\n",
    "The **area under the Precision-Recall curve**, also known as Average Precision \n",
    "(AP), summarizes model performance across thresholds. A model that maintains high \n",
    "precision as recall increases is generally more desirable, especially in settings \n",
    "where false positives have a high cost. This makes the PR curve a complementary \n",
    "and sometimes more informative tool than ROC AUC in skewed classification scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Example 1: Subplot Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first Precision-Recall evaluation example, we plot the PR curves for two \n",
    "models: Logistic Regression and Random Forest Classifier.\n",
    "\n",
    "The curves are arranged side by side using a subplot layout (``n_cols=2, n_rows=1``), \n",
    "with the Logistic Regression curve rendered in blue and the Random Forest curve \n",
    "in green to distinguish between models. A gray dashed line indicates the baseline \n",
    "precision, equal to the prevalence of the positive class in the dataset.\n",
    "\n",
    "This example illustrates how the ``show_pr_curve`` function makes it easy to \n",
    "visualize and compare model performance when dealing with class imbalance. It \n",
    "also demonstrates layout flexibility and customization options, including gridlines, \n",
    "label styling, and export functionality, making it suitable for both exploratory \n",
    "analysis and final reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_pr_curve\n",
    "\n",
    "show_pr_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(12, 6),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Example 2: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second Precision-Recall evaluation example, we focus on overlaying the \n",
    "results of two models: Logistic Regression and Random Forest Classifier onto a single plot. \n",
    "Using the `show_pr_curve` function with the `overlay=True` parameter, the Precision-Recall curves for \n",
    "both models are displayed together, with Logistic Regression in blue and Random \n",
    "Forest in black, both with a `linewidth=2`. The plot includes a custom title \n",
    "for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_pr_curve\n",
    "\n",
    "show_pr_curve(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    decimal_places=2,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    curve_kwgs={\n",
    "        \"Logistic Regression\": {\"color\": \"blue\", \"linewidth\": 2},\n",
    "        \"Random Forest\": {\"color\": \"black\", \"linewidth\": 2},\n",
    "    },\n",
    "    title=\"ROC Curves: Logistic Regression and Random Forest\",\n",
    "    overlay=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Example 3: Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third Precision-Recall evaluation example, we utilize the well-known \n",
    "*Adult Income* dataset, a widely used benchmark for binary classification \n",
    "tasks. Its rich mix of categorical and numerical features makes it particularly \n",
    "suitable for analyzing model performance across different subgroups.\n",
    "\n",
    "To build and evaluate our models, we use the `model_tuner` library. \n",
    "\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html).\n",
    "\n",
    "\n",
    "The objective here is to assess ROC AUC scores not just overall, but \n",
    "**across each category of a selected feature**, such as *occupation*, \n",
    "*education*, *marital-status*, or *race*. This approach enables deeper insight into how \n",
    "performance varies by subgroup, which is particularly important for fairness, \n",
    "bias detection, and subgroup-level interpretability.\n",
    "\n",
    "The `show_pr_curve` function supports this analysis through the \n",
    "`group_category` parameter. \n",
    "\n",
    "For example, by passing `group_category=X_test_2[\"race\"]`, \n",
    "you can generate a separate ROC curve for each unique racial group in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_pr_curve\n",
    "\n",
    "show_pr_curve(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=\"Random Forest Classifier\",\n",
    "    decimal_places=2,\n",
    "    group_category=X_test_2_ai[\"race\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces the `show_confusion_matrix` function, which provides a \n",
    "flexible, styled interface for generating and visualizing confusion matrices \n",
    "across one or more classification models. It supports advanced features like \n",
    "threshold overrides, subgroup labeling, classification report display, and fully \n",
    "customizable plot aesthetics including subplot layouts.\n",
    "\n",
    "The confusion matrix is a fundamental diagnostic tool for classification models, \n",
    "displaying the counts of true positives, true negatives, false positives, and \n",
    "false negatives. This function goes beyond standard implementations by allowing \n",
    "for custom thresholds (globally or per model), label annotation (e.g., TP, FP, etc.), \n",
    "plot exporting, colorbar toggling, and subplot visualization.\n",
    "\n",
    "This is especially useful when comparing multiple models side-by-side or needing \n",
    "publication-ready confusion matrices for stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example 1: Threshold=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    # cmap=\"viridis\",\n",
    "    text_wrap=40,\n",
    "    # title=\"Custom\",\n",
    "    save_plot=True,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    figsize=(6, 6),\n",
    "    show_colorbar=False,\n",
    "    label_fontsize=16,\n",
    "    tick_fontsize=12,\n",
    "    inner_fontsize=14,\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example 2: Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_confusion_matrix\n",
    "\n",
    "show_confusion_matrix(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    cmap=\"viridis\",\n",
    "    text_wrap=40,\n",
    "    subplots=True,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    figsize=(6, 6),\n",
    "    tick_fontsize=14,\n",
    "    inner_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    show_colorbar=True,\n",
    "    class_report=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Example 3: Threshold = 0.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this third confusion matrix evaluation example using the synthetic dataset \n",
    "from the [Binary Classification Models section](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification), we apply \n",
    "a custom classification threshold of 0.37 using the `custom_threshold` parameter. \n",
    "This overrides the default threshold of 0.5 and enables us to inspect how the \n",
    "confusion matrices shift when a more lenient decision boundary is applied. Refer \n",
    "to the section on [threshold selection logic](https://lshpaner.github.io/model_metrics_docs/conceptual_notes.html#threshold-selection-logic)\n",
    "for caveats on choosing the right threshold.\n",
    "\n",
    "This is especially useful in imbalanced classification problems or cost-sensitive \n",
    "environments where the trade-off between precision and recall must be adjusted. \n",
    "By lowering the threshold, we increase the number of positive predictions, \n",
    "which can improve recall but may come at the cost of more false positives.\n",
    "\n",
    "The output matrices for both models: Logistic Regression and Random Forest are shown \n",
    "side by side in a subplot layout for easy visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_confusion_matrix\n",
    "\n",
    "show_confusion_matrix(\n",
    "    model=[model1, model2],\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    model_title=model_titles,\n",
    "    text_wrap=40,\n",
    "    subplots=True,\n",
    "    n_cols=2,\n",
    "    n_rows=1,\n",
    "    figsize=(6, 6),\n",
    "    tick_fontsize=14,\n",
    "    inner_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    custom_threshold=0.37,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on calibration curves, a diagnostic tool that compares \n",
    "predicted probabilities to actual outcomes, helping evaluate how well a model's \n",
    "predicted confidence aligns with observed frequencies. Using models like Logistic \n",
    "Regression or Random Forest on the synthetic dataset from the previous \n",
    "[(Binary Classification Models)](https://lshpaner.github.io/model_metrics_docs/performance_assessment.html#binary-classification) section, we generate \n",
    "calibration curves to assess the reliability of model probabilities.\n",
    "\n",
    "Calibration is especially important in domains where probability outputs inform \n",
    "downstream decisions, such as healthcare, finance, and risk management. A \n",
    "well-calibrated model not only predicts the correct class but also outputs \n",
    "meaningful probabilities, for example, when a model predicts a 0.7 probability, \n",
    "we expect roughly 70% of such predictions to be correct.\n",
    "\n",
    "The `show_calibration_curve` function simplifies this process by allowing users to \n",
    "visualize calibration performance across models or subgroups. The plots show the \n",
    "mean predicted probabilities against the actual observed fractions of positive \n",
    "cases, with an optional reference line representing perfect calibration. \n",
    "Additional features include support for overlay or subplot layouts, subgroup \n",
    "analysis by categorical features, and optional Brier score display, a scalar \n",
    "measure of calibration quality.\n",
    "\n",
    "The function offers full control over styling, figure layout, axis labels, and \n",
    "output format, making it easy to generate both exploratory and publication-ready \n",
    "plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curve Example 1: Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_calibration_curve\n",
    "\n",
    "show_calibration_curve(\n",
    "    model=pipelines_or_models[:2],\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=model_titles[:2],\n",
    "    text_wrap=50,\n",
    "    bins=10,\n",
    "    show_brier_score=True,\n",
    "    figsize=(12, 6),\n",
    "    subplots=True,\n",
    "    linestyle_kwgs={\"color\": \"black\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curve Example 2: Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example also uses the well-known *Adult Income* dataset, a widely used \n",
    "benchmark for binary classification tasks. Its rich mix of categorical and \n",
    "numerical features makes it particularly suitable for analyzing model performance \n",
    "across different subgroups.\n",
    "\n",
    "To train and evaluate the model, we rely on the `model_tuner` library.\n",
    "\n",
    "[Click here to view the corresponding codebase for this workflow](https://lshpaner.github.io/model_metrics_docs/model_training.html#adult-income-training).\n",
    "\n",
    "This example demonstrates how to overlay calibration curves from multiple classification \n",
    "models in a single plot. Overlaying allows for direct visual comparison of how predicted \n",
    "probabilities from each model align with actual outcomes on the same axes.\n",
    "\n",
    "The diagonal dashed line represents perfect calibration, and Brier scores are included \n",
    "in the legend for each model, providing a quantitative measure of calibration accuracy.\n",
    "\n",
    "By setting `overlay=True`, the function combines all model curves into one figure, \n",
    "making it easier to evaluate relative performance without splitting across subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_calibration_curve\n",
    "\n",
    "show_calibration_curve(\n",
    "    model=pipelines_or_models,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=model_titles_ai,\n",
    "    bins=10,\n",
    "    figsize=(14, 10),\n",
    "    show_brier_score=True,\n",
    "    overlay=True,\n",
    "    brier_decimals=4,\n",
    "    linestyle_kwgs={\"color\": \"black\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Curve Example 3: by Category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import show_calibration_curve\n",
    "\n",
    "show_calibration_curve(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X=X_test_ai,\n",
    "    y=y_test_ai,\n",
    "    model_title=\"Random Forest Classifier\",\n",
    "    bins=10,\n",
    "    show_brier_score=True,\n",
    "    brier_decimals=4,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    linestyle_kwgs={\"color\": \"black\"},\n",
    "    curve_kwgs={title: {\"linewidth\": 2} for title in model_titles_ai},\n",
    "    group_category=X_test_2_ai[\"race\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Metric Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces a powerful utility for exploring how classification \n",
    "thresholds affect key performance metrics, including **Precision**, **Recall**, \n",
    "**F1 Score**, and **Specificity**. Rather than fixing a threshold (commonly at 0.5),\n",
    "this function allows users to visualize **trade-offs across the full range of \n",
    "possible thresholds**, making it especially useful when optimizing for use-case-specific \n",
    "goals such as maximizing recall or achieving a minimum precision.\n",
    "\n",
    "Using the Random Forest Classifier models trained on the \n",
    "[adult income dataset](https://lshpaner.github.io/model_metrics_docs/model_training.html), \n",
    "this tool helps users answer practical questions like:\n",
    "\n",
    "- *What threshold achieves at least 85% precision?*\n",
    "- *Where does F1 score peak for this model?*\n",
    "- *How does specificity behave as the threshold increases?*\n",
    "\n",
    "The plot_threshold_metrics function supports optional threshold lookups via \n",
    "`lookup_metric` and `lookup_value`, which prints the closest threshold that\n",
    "meets your constraint. Plots can be customized with colors, gridlines, line styles,\n",
    "wrapped titles, and export options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Curves Example 1: Threshold=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to plot threshold-dependent classification metrics .\n",
    "\n",
    "The `plot_threshold_metrics` function visualizes how Precision, Recall, F1 Score, \n",
    "and Specificity change as the decision threshold varies. In this configuration, \n",
    "the baseline threshold line at 0.5 is enabled (`baseline_thresh=True`), \n",
    "and the line styling is customized via `curve_kwgs`. Font sizes and wrapping options \n",
    "are adjusted for improved clarity in presentation-ready plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import plot_threshold_metrics\n",
    "\n",
    "plot_threshold_metrics(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X_test=X_test_ai,\n",
    "    y_test=y_test_ai,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    baseline_thresh=True,\n",
    "    baseline_kwgs={\n",
    "        \"color\": \"purple\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    curve_kwgs={\n",
    "        \"linestyle\": \"-\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    text_wrap=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Curves Example 2: Targeted Metric Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example expands on threshold-based classification metric visualization using \n",
    "a targeted lookup scenario. Suppose a clinical stakeholder or domain expert has \n",
    "determined (based on prior research, cost-benefit considerations, or operational\n",
    "constraints) that a precision of approximately `0.879` is ideal for downstream \n",
    "decision-making (e.g., minimizing false positives in a healthcare setting).\n",
    "\n",
    "The `plot_threshold_metrics` function accepts the optional arguments `lookup_metric` \n",
    "and `lookup_value` to help identify the threshold that best aligns with this target. \n",
    "When these are set, the function automatically locates and highlights the threshold \n",
    "that most closely achieves the desired metric value, offering transparency and \n",
    "guidance for threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import plot_threshold_metrics\n",
    "\n",
    "plot_threshold_metrics(\n",
    "    model=model_rf[\"model\"].estimator,\n",
    "    X_test=X_test_ai,\n",
    "    y_test=y_test_ai,\n",
    "    lookup_metric=\"precision\",\n",
    "    lookup_value=0.879,\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    baseline_thresh=False,\n",
    "    lookup_kwgs={\n",
    "        \"color\": \"red\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    curve_kwgs={\n",
    "        \"linestyle\": \"-\",\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    text_wrap=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Curves Example 3: Model-Specific Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many production settings, a classifier is deployed with a tuned decision threshold different from the default 0.5 (e.g., to balance costs of false positives vs. false negatives).\n",
    "This example shows how to **explicitly pass a model's chosen threshold** to be drawn as a vertical guide on the plot using `model_threshold=....`\n",
    "You can do this whether you're providing a model/X pair or pre-computed probabilities via `y_prob`. Below we show the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for Random Forest model\n",
    "y_prob_rf = model_rf[\"model\"].estimator.predict_proba(X_test_ai)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model thresholds\n",
    "model_thresholds = {\n",
    "    \"Logistic Regression\": next(iter(model_lr[\"model\"].threshold.values())),\n",
    "    \"Decision Tree Classifier\": next(iter(model_dt[\"model\"].threshold.values())),\n",
    "    \"Random Forest Classifier\": next(iter(model_rf[\"model\"].threshold.values())),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import plot_threshold_metrics\n",
    "\n",
    "# Example: Use precomputed probabilities but still highlight the model's tuned threshold.\n",
    "plot_threshold_metrics(\n",
    "    y_prob=y_prob_rf,  # precomputed probabilities for the positive class\n",
    "    y_test=y_test_ai,  # ground-truth labels\n",
    "    baseline_thresh=False,  # hide the default 0.5 guide\n",
    "    model_threshold=model_thresholds[\"Random Forest Classifier\"],\n",
    "    figsize=(14, 10),\n",
    "    tick_fontsize=14,\n",
    "    label_fontsize=16,\n",
    "    image_path_png=image_path_png,\n",
    "    image_path_svg=image_path_svg,\n",
    "    save_plot=True,\n",
    "    threshold_kwgs={  # styling for the model-threshold vertical line\n",
    "        \"color\": \"blue\",\n",
    "        \"linestyle\": \"--\",\n",
    "        \"linewidth\": 1,\n",
    "    },\n",
    "    curve_kwgs={  # styling for metric curves\n",
    "        \"linestyle\": \"-\",\n",
    "        \"linewidth\": 1.25,\n",
    "    },\n",
    "    text_wrap=40,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
